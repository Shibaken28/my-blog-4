<!doctype html><html lang=ja dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="これは何 強化学習の手法のうちの一つであるDeep Q-Network (DQN)で、ぷよぷよを学習させるという記事です。本記事はガッツリぷよぷよAIについて研究したとかではなく、こんなことやってみました程度のものです。私自身、強化学習について最近興味を持った素人で、「強化学習ってどのくらいできるもんなのか」という動機でやりました。
結果として、サルより強いAIが完成しました
また、本記事は長野高専アドベントカレンダーの2日目の記事です。
DQNとは Deep Q-Network(DQN)をいきなり説明するのは無理なので、次の順で説明します。
用語 Q学習 ニューラルネットワーク 用語 強化学習特有の用語をいくつか説明します。
用語 説明 ぷよぷよでの例 エージェント 学習する本人のこと ぷよぷよを操作する人 状態 エージェントに与える状況 盤面の状態 行動 エージェントが取れる行動 ぷよを置く場所と向き 報酬 エージェントが行動を取ったときに与えられる値 ぷよを消した数など エージェントは、報酬の総和(正確には、総和ではないこともある)を最大化するように行動を選択します。ですので、学習の目的によって報酬の付け方が変わります。
報酬は、各行動を一回行うごとに与えられます。つまり、次のサイクルで学習が行われます。
エージェントが状態を観測する エージェントが行動を選択する エージェントが行動を実行し、状態が遷移する エージェントが報酬を受け取る 1に戻る また、報酬は負の値にすることも可能です。例えば、ゲームオーバーになってしまった場合には報酬$-1$を与える、ということもできます。
Q学習 定義 $Q$学習は$Q$関数を用います。$Q$関数は次のように定義されます。
$$ Q(s,a) = (状態sで行動aを取ったときの評価値) $$
シンプルな定義です。「評価値」は大きければ大きいほど、その先で得られる報酬が大きいということを表します。
もし、完全な$Q$関数が完成していたとしましょう。このときは、次の手順で最適な行動ができます。
現在の状態$s$で行える全ての行動$a_1,a_2,\dots,a_n$について、$Q(s,a_i)$を計算する $Q(s,a_i)$が最大となる行動$a_i$を選択する Q関数の作り方 ランダムに行動して、その結果得られた報酬を$Q$関数に反映させることで、$Q$関数の各値を求めます。
ある状態$s$で行動$a$を取ったときに、状態$s^\prime$に遷移し、報酬$r$を得たとします。このとき、$Q$関数の値を次のように更新します。
$$ Q(s,a) \leftarrow Q(s,a) + \alpha(r + \gamma \max_{a^\prime}Q(s^\prime,a^\prime) - Q(s,a)) $$
ここで、$\alpha$は学習率、$\gamma$は割引率と呼ばれる値です(ただし$0\leq \alpha,\gamma \leq 1$)。
この式は実はそんなに難しいことは言っていないのですが、説明するのが面倒なので省略します。
理屈はさておき、この更新式を使って、ランダムに動くことを繰り返して$Q$関数を求めることはできます。しかし、状態$s$の場合の数が非常に大きいな場合、$Q$関数を求めるのに時間がかかってしまいます。例えば、ぷよぷよの盤面の場合、縦$12$横$6$の計$72$マスがあり、各マスは$4$色のぷよか、空白のいずれかであるため、$5^{72}$通りの状態があります1。これは約$10^{50}$通りであり、現時点で世界に存在するコンピューターで計算するのは不可能なオーダーです。"><title>強化学習でぷよぷよを学習させたかった(願望)</title><link rel=canonical href=https://shibaken28.github.io/my-blog-4/contents/puyo-dqn/><link rel=stylesheet href=/my-blog-4/scss/style.min.abbd69b2908fdfcd5179898beaafd374514a86538d81639ddd2c58c06ae54e40.css><meta property="og:title" content="強化学習でぷよぷよを学習させたかった(願望)"><meta property="og:description" content="これは何 強化学習の手法のうちの一つであるDeep Q-Network (DQN)で、ぷよぷよを学習させるという記事です。本記事はガッツリぷよぷよAIについて研究したとかではなく、こんなことやってみました程度のものです。私自身、強化学習について最近興味を持った素人で、「強化学習ってどのくらいできるもんなのか」という動機でやりました。
結果として、サルより強いAIが完成しました
また、本記事は長野高専アドベントカレンダーの2日目の記事です。
DQNとは Deep Q-Network(DQN)をいきなり説明するのは無理なので、次の順で説明します。
用語 Q学習 ニューラルネットワーク 用語 強化学習特有の用語をいくつか説明します。
用語 説明 ぷよぷよでの例 エージェント 学習する本人のこと ぷよぷよを操作する人 状態 エージェントに与える状況 盤面の状態 行動 エージェントが取れる行動 ぷよを置く場所と向き 報酬 エージェントが行動を取ったときに与えられる値 ぷよを消した数など エージェントは、報酬の総和(正確には、総和ではないこともある)を最大化するように行動を選択します。ですので、学習の目的によって報酬の付け方が変わります。
報酬は、各行動を一回行うごとに与えられます。つまり、次のサイクルで学習が行われます。
エージェントが状態を観測する エージェントが行動を選択する エージェントが行動を実行し、状態が遷移する エージェントが報酬を受け取る 1に戻る また、報酬は負の値にすることも可能です。例えば、ゲームオーバーになってしまった場合には報酬$-1$を与える、ということもできます。
Q学習 定義 $Q$学習は$Q$関数を用います。$Q$関数は次のように定義されます。
$$ Q(s,a) = (状態sで行動aを取ったときの評価値) $$
シンプルな定義です。「評価値」は大きければ大きいほど、その先で得られる報酬が大きいということを表します。
もし、完全な$Q$関数が完成していたとしましょう。このときは、次の手順で最適な行動ができます。
現在の状態$s$で行える全ての行動$a_1,a_2,\dots,a_n$について、$Q(s,a_i)$を計算する $Q(s,a_i)$が最大となる行動$a_i$を選択する Q関数の作り方 ランダムに行動して、その結果得られた報酬を$Q$関数に反映させることで、$Q$関数の各値を求めます。
ある状態$s$で行動$a$を取ったときに、状態$s^\prime$に遷移し、報酬$r$を得たとします。このとき、$Q$関数の値を次のように更新します。
$$ Q(s,a) \leftarrow Q(s,a) + \alpha(r + \gamma \max_{a^\prime}Q(s^\prime,a^\prime) - Q(s,a)) $$
ここで、$\alpha$は学習率、$\gamma$は割引率と呼ばれる値です(ただし$0\leq \alpha,\gamma \leq 1$)。
この式は実はそんなに難しいことは言っていないのですが、説明するのが面倒なので省略します。
理屈はさておき、この更新式を使って、ランダムに動くことを繰り返して$Q$関数を求めることはできます。しかし、状態$s$の場合の数が非常に大きいな場合、$Q$関数を求めるのに時間がかかってしまいます。例えば、ぷよぷよの盤面の場合、縦$12$横$6$の計$72$マスがあり、各マスは$4$色のぷよか、空白のいずれかであるため、$5^{72}$通りの状態があります1。これは約$10^{50}$通りであり、現時点で世界に存在するコンピューターで計算するのは不可能なオーダーです。"><meta property="og:url" content="https://shibaken28.github.io/my-blog-4/contents/puyo-dqn/"><meta property="og:site_name" content="shibak3n's blog"><meta property="og:type" content="article"><meta property="article:section" content="Post"><meta property="article:tag" content="DQN"><meta property="article:tag" content="強化学習"><meta property="article:published_time" content="2023-12-02T12:08:30+09:00"><meta property="article:modified_time" content="2023-12-02T12:08:30+09:00"><meta name=twitter:title content="強化学習でぷよぷよを学習させたかった(願望)"><meta name=twitter:description content="これは何 強化学習の手法のうちの一つであるDeep Q-Network (DQN)で、ぷよぷよを学習させるという記事です。本記事はガッツリぷよぷよAIについて研究したとかではなく、こんなことやってみました程度のものです。私自身、強化学習について最近興味を持った素人で、「強化学習ってどのくらいできるもんなのか」という動機でやりました。
結果として、サルより強いAIが完成しました
また、本記事は長野高専アドベントカレンダーの2日目の記事です。
DQNとは Deep Q-Network(DQN)をいきなり説明するのは無理なので、次の順で説明します。
用語 Q学習 ニューラルネットワーク 用語 強化学習特有の用語をいくつか説明します。
用語 説明 ぷよぷよでの例 エージェント 学習する本人のこと ぷよぷよを操作する人 状態 エージェントに与える状況 盤面の状態 行動 エージェントが取れる行動 ぷよを置く場所と向き 報酬 エージェントが行動を取ったときに与えられる値 ぷよを消した数など エージェントは、報酬の総和(正確には、総和ではないこともある)を最大化するように行動を選択します。ですので、学習の目的によって報酬の付け方が変わります。
報酬は、各行動を一回行うごとに与えられます。つまり、次のサイクルで学習が行われます。
エージェントが状態を観測する エージェントが行動を選択する エージェントが行動を実行し、状態が遷移する エージェントが報酬を受け取る 1に戻る また、報酬は負の値にすることも可能です。例えば、ゲームオーバーになってしまった場合には報酬$-1$を与える、ということもできます。
Q学習 定義 $Q$学習は$Q$関数を用います。$Q$関数は次のように定義されます。
$$ Q(s,a) = (状態sで行動aを取ったときの評価値) $$
シンプルな定義です。「評価値」は大きければ大きいほど、その先で得られる報酬が大きいということを表します。
もし、完全な$Q$関数が完成していたとしましょう。このときは、次の手順で最適な行動ができます。
現在の状態$s$で行える全ての行動$a_1,a_2,\dots,a_n$について、$Q(s,a_i)$を計算する $Q(s,a_i)$が最大となる行動$a_i$を選択する Q関数の作り方 ランダムに行動して、その結果得られた報酬を$Q$関数に反映させることで、$Q$関数の各値を求めます。
ある状態$s$で行動$a$を取ったときに、状態$s^\prime$に遷移し、報酬$r$を得たとします。このとき、$Q$関数の値を次のように更新します。
$$ Q(s,a) \leftarrow Q(s,a) + \alpha(r + \gamma \max_{a^\prime}Q(s^\prime,a^\prime) - Q(s,a)) $$
ここで、$\alpha$は学習率、$\gamma$は割引率と呼ばれる値です(ただし$0\leq \alpha,\gamma \leq 1$)。
この式は実はそんなに難しいことは言っていないのですが、説明するのが面倒なので省略します。
理屈はさておき、この更新式を使って、ランダムに動くことを繰り返して$Q$関数を求めることはできます。しかし、状態$s$の場合の数が非常に大きいな場合、$Q$関数を求めるのに時間がかかってしまいます。例えば、ぷよぷよの盤面の場合、縦$12$横$6$の計$72$マスがあり、各マスは$4$色のぷよか、空白のいずれかであるため、$5^{72}$通りの状態があります1。これは約$10^{50}$通りであり、現時点で世界に存在するコンピューターで計算するのは不可能なオーダーです。"><script async src="https://www.googletagmanager.com/gtag/js?id=G-KXVWMJ76L6"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KXVWMJ76L6")</script><link href="https://fonts.googleapis.com/css?family=Noto+Sans+JP" rel=stylesheet><style>:root{--ja-font-family:"Noto Sans JP",'ヒラギノ角ゴ Pro W3', 'Hiragino Kaku Gothic Pro', '游ゴシック', 'Yu Gothic', 'ＭＳ Ｐゴシック', 'MS PGothic', sans-serif;--base-font-family:"Lato", var(--sys-font-family), var(--ja-font-family), sans-serif;--article-font-size:12pt}.article-content .katex-display>.katex{overflow-x:hidden;overflow-y:hidden}</style></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label=メニューを開く・閉じる>
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><div class=site-meta><h1 class=site-name><a href=/my-blog-4>shibak3n's blog</a></h1><h2 class=site-description>趣味全般</h2></div></header><ol class=social-menu><li><a href=https://github.com/Shibaken28 target=_blank title=GitHub rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=https://steamcommunity.com/profiles/76561199155194438/ target=_blank title=steam rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg></a></li><li><a href=https://twitter.com/Shibak33333333n target=_blank title=Twitter rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-twitter" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M22 4.01c-1 .49-1.98.689-3 .99-1.121-1.265-2.783-1.335-4.38-.737S11.977 6.323 12 8v1c-3.245.083-6.135-1.395-8-4 0 0-4.182 7.433 4 11-1.872 1.247-3.739 2.088-6 2 3.308 1.803 6.913 2.423 10.034 1.517 3.58-1.04 6.522-3.723 7.651-7.742a13.84 13.84.0 00.497-3.753C20.18 7.773 21.692 5.25 22 4.009z"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/my-blog-4/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg><span>Home</span></a></li><li><a href=/my-blog-4/archives/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg><span>Archives</span></a></li><li><a href=/my-blog-4/search/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg><span>Search</span></a></li><li><a href=/my-blog-4/links/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg><span>Links</span></a></li><div class=menu-bottom-section><li id=dark-mode-toggle><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><span>ダークモード</span></li></div></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">目次</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#これは何>これは何</a></li><li><a href=#dqnとは>DQNとは</a><ol><li><a href=#用語>用語</a></li><li><a href=#q学習>Q学習</a><ol><li><a href=#定義>定義</a></li><li><a href=#q関数の作り方>Q関数の作り方</a></li></ol></li><li><a href=#ニューラルネットワーク>ニューラルネットワーク</a></li></ol></li><li><a href=#ぷよぷよの実装>ぷよぷよの実装</a></li><li><a href=#学習方法>学習方法</a><ol><li><a href=#状態>状態</a></li><li><a href=#epsilon-greedy法>epsilon-greedy法</a></li></ol></li><li><a href=#学習>学習</a><ol><li><a href=#実験a-生き残れるか>実験A 生き残れるか</a><ol><li><a href=#条件>条件</a></li><li><a href=#結果>結果</a></li></ol></li><li><a href=#実験b-2連鎖以上を起こせるか>実験B 2連鎖以上を起こせるか</a><ol><li><a href=#条件-1>条件</a></li><li><a href=#結果-1>結果</a></li></ol></li><li><a href=#実験c-ばよえん>実験C ばよえ～～ん</a><ol><li><a href=#条件-2>条件</a></li><li><a href=#結果-2>結果</a></li></ol></li></ol></li><li><a href=#考察>考察</a></li><li><a href=#感想と謝罪文と言い訳>感想と謝罪文と言い訳</a></li></ol></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/my-blog-4/categories/%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92/>強化学習</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/my-blog-4/contents/puyo-dqn/>強化学習でぷよぷよを学習させたかった(願望)</a></h2></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg><time class=article-time--published>Dec 02, 2023</time></div></footer></div></header><section class=article-content><h2 id=これは何>これは何</h2><p>強化学習の手法のうちの一つであるDeep Q-Network (DQN)で、ぷよぷよを学習させるという記事です。本記事はガッツリぷよぷよAIについて研究したとかではなく、こんなことやってみました程度のものです。私自身、強化学習について最近興味を持った素人で、「強化学習ってどのくらいできるもんなのか」という動機でやりました。</p><p><strong>結果として、サルより強いAIが完成しました</strong></p><p>また、本記事は<a class=link href=https://qiita.com/advent-calendar/2023/nnct target=_blank rel=noopener>長野高専アドベントカレンダー</a>の2日目の記事です。</p><h2 id=dqnとは>DQNとは</h2><p>Deep Q-Network(DQN)をいきなり説明するのは無理なので、次の順で説明します。</p><ol><li>用語</li><li>Q学習</li><li>ニューラルネットワーク</li></ol><h3 id=用語>用語</h3><p>強化学習特有の用語をいくつか説明します。</p><div class=table-wrapper><table><thead><tr><th style=text-align:left>用語</th><th style=text-align:left>説明</th><th style=text-align:left>ぷよぷよでの例</th></tr></thead><tbody><tr><td style=text-align:left>エージェント</td><td style=text-align:left>学習する本人のこと</td><td style=text-align:left>ぷよぷよを操作する人</td></tr><tr><td style=text-align:left>状態</td><td style=text-align:left>エージェントに与える状況</td><td style=text-align:left>盤面の状態</td></tr><tr><td style=text-align:left>行動</td><td style=text-align:left>エージェントが取れる行動</td><td style=text-align:left>ぷよを置く場所と向き</td></tr><tr><td style=text-align:left>報酬</td><td style=text-align:left>エージェントが行動を取ったときに与えられる値</td><td style=text-align:left>ぷよを消した数など</td></tr></tbody></table></div><p>エージェントは、報酬の総和(正確には、総和ではないこともある)を最大化するように行動を選択します。ですので、学習の目的によって報酬の付け方が変わります。</p><p>報酬は、各行動を一回行うごとに与えられます。つまり、次のサイクルで学習が行われます。</p><ol><li>エージェントが状態を観測する</li><li>エージェントが行動を選択する</li><li>エージェントが行動を実行し、状態が遷移する</li><li>エージェントが報酬を受け取る</li><li>1に戻る</li></ol><p>また、報酬は負の値にすることも可能です。例えば、ゲームオーバーになってしまった場合には報酬$-1$を与える、ということもできます。</p><h3 id=q学習>Q学習</h3><h4 id=定義>定義</h4><p>$Q$学習は$Q$関数を用います。$Q$関数は次のように定義されます。</p><p>$$
Q(s,a) = (状態sで行動aを取ったときの評価値)
$$</p><p>シンプルな定義です。「評価値」は大きければ大きいほど、その先で得られる報酬が大きいということを表します。</p><p>もし、完全な$Q$関数が完成していたとしましょう。このときは、次の手順で最適な行動ができます。</p><ol><li>現在の状態$s$で行える全ての行動$a_1,a_2,\dots,a_n$について、$Q(s,a_i)$を計算する</li><li>$Q(s,a_i)$が最大となる行動$a_i$を選択する</li></ol><h4 id=q関数の作り方>Q関数の作り方</h4><p>ランダムに行動して、その結果得られた報酬を$Q$関数に反映させることで、$Q$関数の各値を求めます。</p><p>ある状態$s$で行動$a$を取ったときに、状態$s^\prime$に遷移し、報酬$r$を得たとします。このとき、$Q$関数の値を次のように更新します。</p><p>$$
Q(s,a) \leftarrow Q(s,a) + \alpha(r + \gamma \max_{a^\prime}Q(s^\prime,a^\prime) - Q(s,a))
$$</p><p>ここで、$\alpha$は学習率、$\gamma$は割引率と呼ばれる値です(ただし$0\leq \alpha,\gamma \leq 1$)。</p><p>この式は実はそんなに難しいことは言っていないのですが、説明するのが面倒なので省略します。</p><p>理屈はさておき、この更新式を使って、ランダムに動くことを繰り返して$Q$関数を求めることはできます。しかし、状態$s$の場合の数が非常に大きいな場合、$Q$関数を求めるのに時間がかかってしまいます。例えば、ぷよぷよの盤面の場合、縦$12$横$6$の計$72$マスがあり、各マスは$4$色のぷよか、空白のいずれかであるため、$5^{72}$通りの状態があります<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>。これは約$10^{50}$通りであり、現時点で世界に存在するコンピューターで計算するのは不可能なオーダーです。</p><p>そこで、ニューラルネットワークの出番です。</p><h3 id=ニューラルネットワーク>ニューラルネットワーク</h3><p>簡単に説明すると関数を予測する構造を持ちます。例えるならば、グラフに点をプロットしていくと、その点を通るような関数を予測してくれます。ニューラルネットワークの中では、偏微分やら何やらを使って(誤差逆伝播法やBackPropagationで調べると出てきます)、誤差が小さくなるようにパラメータが調節されます。</p><figure><img src=./img/net.jpg width=80%><figcaption><h4>ニューラルネットワークのイメージ</h4></figcaption></figure><p>ちなみにこの図ですが、卒研の中間発表の際に使ったものを持ったきたものです。中間発表は良い思い出ではないので、あまりこの図を見たくありません。</p><p>これを使うことで、全パターンの状態を試さなくても、$Q$関数の値を予測してくれるんじゃないか、というのがDeep Q-Network(DQN)のアイデアです。</p><h2 id=ぷよぷよの実装>ぷよぷよの実装</h2><p>ぷよぷよを学習させるためにはぷよぷよのシステムを作らなければなりません。作りました。</p><p>DQNにおいて必要なのは「行動を与えたときに、次の状態と報酬を返すメソッド」です。これさえあればDQNに突っ込めば学習ができます。<a class=link href=https://www.gymlibrary.dev/ target=_blank rel=noopener>OpenAIGym</a>に倣って、そのゲームの「環境」クラスをインスタンス化して使えるようにします。</p><p>以下は主要部分だけ抜粋したものです。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>step</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>action</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>reward</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>    <span class=n>done</span> <span class=o>=</span> <span class=kc>False</span>
</span></span><span class=line><span class=cl>    <span class=n>win</span> <span class=o>=</span> <span class=kc>False</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>prev_action</span> <span class=o>=</span> <span class=n>action</span>
</span></span><span class=line><span class=cl>    <span class=n>puyo</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>puyo_list</span><span class=p>[</span><span class=bp>self</span><span class=o>.</span><span class=n>turn</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>turn</span> <span class=o>+=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>isDropped</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>drop_puyo</span><span class=p>(</span><span class=n>action</span><span class=p>,</span> <span class=n>puyo</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=ow>not</span> <span class=n>isDropped</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=c1># 落とせなかったら負け</span>
</span></span><span class=line><span class=cl>        <span class=n>done</span> <span class=o>=</span> <span class=kc>True</span>
</span></span><span class=line><span class=cl>        <span class=n>reward</span> <span class=o>=</span> <span class=o>-</span><span class=mi>1</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>_dis</span> <span class=p>,</span> <span class=n>_chain</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>process</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>turn</span> <span class=o>&gt;=</span> <span class=bp>self</span><span class=o>.</span><span class=n>turn_max</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=c1># 全ターン経過で勝ち</span>
</span></span><span class=line><span class=cl>            <span class=n>done</span> <span class=o>=</span> <span class=kc>True</span>
</span></span><span class=line><span class=cl>            <span class=n>win</span> <span class=o>=</span> <span class=kc>True</span>
</span></span><span class=line><span class=cl>            <span class=n>reward</span> <span class=o>=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>    <span class=c1># print(self.board)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>states</span><span class=p>(),</span> <span class=n>reward</span><span class=p>,</span> <span class=n>done</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>info</span><span class=p>(),</span> <span class=n>win</span>
</span></span></code></pre></td></tr></table></div></div><p><code>step()</code>が「行動を与えたときに次の状態と報酬を返す」メソッドです。</p><h2 id=学習方法>学習方法</h2><p><a class=link href=https://github.com/oreilly-japan/deep-learning-from-scratch-4/blob/master/pytorch/dqn.py target=_blank rel=noopener>『ゼロから作るDeep Learning ❹ 強化学習編』(オライリー・ジャパン)のサポートサイトのコード</a>をベースに書きました。機械学習ライブラリのPyTorchを使っています。</p><h3 id=状態>状態</h3><p>状態としてエージェントに与えたい要素は次の通りです。</p><ul><li>盤面の状態</li><li>次に落とすぷよ$3$つ</li></ul><p>これをもとに状態を返すメソッドを書きます。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>states</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># 現在の盤面の状態</span>
</span></span><span class=line><span class=cl>    <span class=n>state</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>row</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>height</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>col</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>width</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>state</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>board</span><span class=p>[</span><span class=n>row</span><span class=p>][</span><span class=n>col</span><span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=c1># 次の3ターンで降るぷよの色</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>3</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>state</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>puyo_list</span><span class=p>[</span><span class=bp>self</span><span class=o>.</span><span class=n>turn</span><span class=o>+</span><span class=n>i</span><span class=p>][</span><span class=mi>0</span><span class=p>])</span>
</span></span><span class=line><span class=cl>        <span class=n>state</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>puyo_list</span><span class=p>[</span><span class=bp>self</span><span class=o>.</span><span class=n>turn</span><span class=o>+</span><span class=n>i</span><span class=p>][</span><span class=mi>1</span><span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=c1># print(state)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>assert</span> <span class=nb>len</span><span class=p>(</span><span class=n>state</span><span class=p>)</span> <span class=o>==</span> <span class=n>STATE_SPACE</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># dtypeをfloat32に変換</span>
</span></span><span class=line><span class=cl>    <span class=n>state</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=n>state</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>state</span>
</span></span></code></pre></td></tr></table></div></div><p>これらをすべて1次元ベクトルとして入力にします。$2$次元の盤面をどう$1$次元に変換させるのかというと、単純に各行を横につなげただけです。</p><h3 id=epsilon-greedy法>epsilon-greedy法</h3><p>epsilon-greedy法は学習テクニックの一つで、学習初期にはランダムに行動を選択し、学習が進むにつれてランダムに行動を選択する確率を下げていく方法です。これを使うことで、学習初期にはランダムに行動を選択することで、より多くの状態を試し、偏りなく学習させることが期待されます。</p><h2 id=学習>学習</h2><p>ソースコードは<a class=link href=https://gist.github.com/Shibaken28/5b794fc0ad75ed42e5402b6a8a8135ef target=_blank rel=noopener>Gist</a>に置いてあります。</p><h3 id=実験a-生き残れるか>実験A 生き残れるか</h3><h4 id=条件>条件</h4><p>次の条件で学習させます。</p><ul><li><p>盤面の大きさは縦$8$横$6$の計$48$マス</p></li><li><p>$30$回ぷよを落とすことができればゲームクリア</p><ul><li>少なくとも、$30\times 2=60$個のうち$60-48=12$個は消さなければならない</li></ul></li><li><p>ゲームクリアの場合報酬$1$、ゲームオーバーの場合報酬$-1$を与える</p></li></ul><h4 id=結果>結果</h4><p>$2000$回の試行を行ったところ、最終的には$4$から$6$割程度の成功率となりました。<figure><img src=./img/servive.png width=60%><figcaption><h4>実験Aの直近100回の成功率</h4></figcaption></figure>なお、$1000$回らへんで急激に成功率が上がっているのは、epsilon-greedy法の影響です。学習初期にはランダムに行動を選択する確率が高いため、成功率が極端に低くなっています。</p><p>学習の効果が出ているのかを確かめるため、試しに全てランダムに行動させてみました。すると、成功率は$1$割にも満たない結果となりました。
よって、精度はともかく、少なくとも学習はできていると言えます。サルより強い。<figure><img src=./img/servive-saru.png width=60%><figcaption><h4>実験Aの直近100回の成功率(ランダム)</h4></figcaption></figure></p><h3 id=実験b-2連鎖以上を起こせるか>実験B 2連鎖以上を起こせるか</h3><h4 id=条件-1>条件</h4><p>次の条件で学習させます。</p><ul><li>盤面の大きさは縦$8$横$6$の計$48$マス</li><li>$2$連鎖以上を起こした時点でゲームクリア</li><li>ただし、$30$回ぷよを落とした時点で$2$連鎖以上が起こっていなかった場合ゲームオーバー</li><li>ゲームクリアの場合報酬$1$、ゲームオーバーの場合報酬$-1$を与える</li></ul><h4 id=結果-1>結果</h4><p>成功率は案外伸びずに2割程度でした。<figure><img src=./img/2.png width=60%><figcaption><h4>実験Bの直近100回の成功率</h4></figcaption></figure>一応ランダムの場合は成功率が1割未満なため、学習自体はできているようです。<figure><img src=./img/2-saru.png width=60%><figcaption><h4>実験Bの直近100回の成功率(ランダム)</h4></figcaption></figure></p><h3 id=実験c-ばよえん>実験C ばよえ～～ん</h3><h4 id=条件-2>条件</h4><p>次の条件で学習させます。</p><ul><li>盤面の大きさは縦$12$横$6$の計$96$マス</li><li>$7$連鎖を起こした時点でゲームクリア</li><li>ただし、$100$回ぷよを落とした時点で$7$連鎖以上が起こっていなかった場合ゲームオーバー</li><li>報酬は、起こした連鎖数の$2$乗<ul><li>これにより、「$1$連鎖$2$回」よりも「$2$連鎖$1$回」の方が良く評価される</li></ul></li></ul><p>要するに<a class=link href=https://dic.nicovideo.jp/a/%E3%81%B0%E3%82%88%E3%81%88%E3%80%9C%E3%82%93 target=_blank rel=noopener>ばよえ～ん</a>を唱えられればクリアです</p><h4 id=結果-2>結果</h4><p>$5000$回の試行のうち、$11$回成功しました。成功率は非常に低いです。</p><p>$1600$回目の試行で$7$連鎖を起こすことができました。</p><iframe src=https://www.pndsng.com/puyo/view.html?a11dadedbdbdbceadecbeadcdebae2dbcacdb2cabcdcdaecb2cd2ecbedc2bec2bceb2d width=256 height=540 frameborder=0 scrolling=no style="transform:scale(2);-o-transform:scale(2);-webkit-transform:scale(2);-moz-transform:scale(2);-ms-transform:scale(2);transform-origin:0 0;-o-transform-origin:0 0;-webkit-transform-origin:0 0;-moz-transform-origin:0 0;-ms-transform-origin:0 0"></iframe><p>別のパターン($2593$回目)。※ゲームオーバーの判定は原作と少し違い、「盤面の1番上の行にぷよが既に存在していて、さらにその上に置こうとするとゲームオーバー」となっています。</p><iframe src=https://www.pndsng.com/puyo/view.html?a7d2a2dae3aea2cdaea2ce2ba2dbdca2ececa2ecbeaeb2dcabd2edac2edead2ecd2e2dec width=256 height=540 frameborder=0 scrolling=no style="transform:scale(2);-o-transform:scale(2);-webkit-transform:scale(2);-moz-transform:scale(2);-ms-transform:scale(2);transform-origin:0 0;-o-transform-origin:0 0;-webkit-transform-origin:0 0;-moz-transform-origin:0 0;-ms-transform-origin:0 0"></iframe><p>$4905$回目の試行。こちらは$8$連鎖起こっていますし、残ったぷよの数が少なめです。</p><iframe src=https://www.pndsng.com/puyo/view.html?a6baeba2caeda4deba3bcea2ecbca2cdecabcecdbecedebd3ce2bc2ecb2d2b2de2dc2e width=256 height=540 frameborder=0 scrolling=no style="transform:scale(2);-o-transform:scale(2);-webkit-transform:scale(2);-moz-transform:scale(2);-ms-transform:scale(2);transform-origin:0 0;-o-transform-origin:0 0;-webkit-transform-origin:0 0;-moz-transform-origin:0 0;-ms-transform-origin:0 0"></iframe><p>報酬の総和の平均は次のようになりました。<figure><img src=./img/7.png width=60%><figcaption><h4>実験Bの直近100回の報酬の総和の平均</h4></figcaption></figure></p><h2 id=考察>考察</h2><p>成功率がそこまで高くならないものの、ランダムの場合よりは良い結果が得られているのは確かではあります。ただ、ぷよぷよというゲームの性質上、次のような行動をすれば案外連鎖を起こせてしまいます。</p><ul><li>盤面から溢れない程度にぷよを積んでおく</li><li>盤面がほとんどぷよでいっぱいで、ゲームオーバーになりそうな場合は適当なぷよを消す</li><li>すると、それなりに連鎖が起こる場合がある</li></ul><p>いわゆる「カエル積み」です。強化学習によって、「盤面から溢れない程度に置く」「満杯になったら適当に消す」ことが良いと認識され、「カエル積み」が生まれているのではないか、という考えです。</p><p>実験Cではカエル積みが何度も試行され、試行回数を大量に重ねた結果、$7$連鎖を起こすことができたと考えます。</p><h2 id=感想と謝罪文と言い訳>感想と謝罪文と言い訳</h2><p>遅れてしまいました。ごめんなさい！！！！</p><p>元々この記事はZennに投稿する予定だったのですが、学習の結果があんまりぱっとしなかったことや、強化学習への理解が足りないことなどの理由から、記事のクオリティが低くなりそうだったので、個人ブログに投げることにしました。個人ブログは何をやっても許されるので便利ですね。</p><p>感想ですが、機械学習は難しいですね。私が普段触れている競プロやアルゴリズムは、厳密さや再現性が重要視される分野であることに対し、機械学習は「ニューラルネットワークを使うとなんかうまくいく」とか、「パラメータを調節すると学習率が上がる」みたいな、どこかふんわりとした印象があります。手元で調整したパラメータが、直接結果に現れるのではなく、「ニューラルネットワーク」という得体の知れないものを通して出てくる、謎の仲介者がいる、直接やり取りしたいのに、という気持ちになります。そこがかなりとっつきづく、私はまだまだ機械学習について理解が足りないと感じました。</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>空白の上にぷよが置かれる場合が含まれてしまっているため、正確にはもうちょっと少ないですが、大きい数であることには間違いないです。&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></section><footer class=article-footer><section class=article-tags><a href=/my-blog-4/tags/dqn/>DQN</a>
<a href=/my-blog-4/tags/%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92/>強化学習</a></section><section class=article-copyright><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg><span>Licensed under CC BY-NC-SA 4.0</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/katex.min.css integrity="sha256-J+iAE0sgH8QSz9hpcDxXIftnj65JEZgNhGcgReTTK9s=" crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/katex.min.js integrity="sha256-InsNdER1b2xUewP+pKCUJpkhiqwHgqiPXDlIk7GzBu4=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/contrib/auto-render.min.js integrity="sha256-y39Mpg7V3D4lhBX4x6O0bUqTV4pSrfgwEfGKfxkOdgI=" crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{renderMathInElement(document.querySelector(`.article-content`),{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><aside class=related-content--wrapper><h2 class=section-title>関連するコンテンツ</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/my-blog-4/contents/rl-0/><div class=article-details><h2 class=article-title>強化学習勉強メモ (目次)</h2></div></a></article></div></div></aside><footer class=site-footer><section class=copyright>&copy;
2022 -
2024 shibak3n's blog</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>テーマ <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.21.0>Stack</a></b> は <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a> によって設計されています。</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/my-blog-4/ts/main.js defer></script>
<script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>
<!doctype html><html lang=ja dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="モンテカルロ法の説明 動的計画法の問題点 動的計画法は，状態遷移確率($p$)と報酬関数($r$)が既知である必要があった．他にも，エピソードの途中の状態を再現しなければならない点も厄介であった．
モンテカルロ法のアイデア モンテカルロ法のアイデアは，全ての行動を試すのが無理ならば，いくつかサンプリングを行って，それの平均値を取ることで，期待値を近似しようというものである1. 例えば，$10$個のサイコロを振ったときの出る目の合計の期待値を解析的に求めるには，$6^{10}=60466176$通りの計算をしなければならない．モンテカルロ法によれば，実際に$10$個のサイコロを振る動作を適当な回数(例えば$1000$回)くらい繰り返して，出た目の平均を取れば十分近似できる．
余談ではあるが，モンテカルロ法は強化学習の分野以外でも使われる．例えば，平面上にランダムな点をたくさん打って，円の中に入っている点の数を数えることで円周率を近似する手法2も，モンテカルロ法と呼ばれている．
状態価値関数をモンテカルロ法で求める 状態$s$における価値$v(s)$を求めたい場合は，状態$s$から始まるエピソードをたくさん生成し，その平均を取れば良い．これはエピソードタスクでのみ可能で，連続タスクには適用できない． エピソード$1$の収益が$G_1$，エピソード$2$の収益が$G_2$，$\cdots$，エピソード$N$の収益が$G_N$とすると，状態$s$における価値$v(s)$は式$(1)$のようになる．
$$ \begin{align} v(s) = \frac{G_1 + G_2 + \cdots + G_N}{N} \end{align} $$
最適方策を見つける 動的計画法と同じく，状態価値関数$V$から方策を改善する方策制御のターンが必要である． 動的計画法と同じく，greedy化を行って方策制御を行うことができる．箇条書きにすると次のようになる．
初期の適当な方策$\pi$を決める 十分な回数次を繰り返す． モンテカルロ法によりエピソードを$1$回通す greedy化を行い$\pi$を更新する(方策制御) ただし，このままだといくつか問題点があるため，次の手法で改善する．
行動価値関数を求める 状態価値関数$v(s)$を求めていたが，これを行動価値関数$q$に変更する．状態価値関数から最適方策を求めようとすると，式$(2)$のようになる． $$ \begin{align} \mu(s) &amp;amp;= \text{argmax} _{a} \sum _{s&amp;rsquo;} p(s&amp;rsquo;|s,a) \left\{ r(s,a,s&amp;rsquo;) + \gamma v (s&amp;rsquo;) \right\} \end{align} $$ これには，$v$に加えて，$p,r$の情報が必要になってしまう．$p,r$を直接求めることが困難な場合，最適方策がわからない．そもそも，$p,r$を直接求めることが困難であったから，ランダムにエピソードを走らせるモンテカルロ法をしているのであり，ここで$p,r$が必要になってしまったら本末転倒である．
代わりに$q$関数を使うとどうだろうか．式$(3)$のように$p$も$r$も必要なく非常にシンプルである！ $$ \begin{align} \mu(s) &amp;amp;= \text{argmax} _{a} q (s,a) \end{align} $$
求めるものが$v$から$q$へと変わったが，モンテカルロ法の適用のさせ方はほとんど変わらず，始めの状態$s$に加えて最初の行動$a$を記憶しておくだけである．
$\varepsilon$-greedy法 (方策評価フェーズ) モンテカルロ法で方策の評価をした後，単にgreedy化し，それを使ってまたモンテカルロ法を行うのは危険である． なぜなら，モンテカルロ法はあくまでサンプリングをしているのであり，収益が悪いサンプルを引いてしまうとgreedy化しても悪い方策が出てくるだけである．そこで，greedy化された方策に加え，たまにランダムな動きをするように修正を加えると，微妙な局所解に陥ってしまうことを逃れられる． 具体的には，確率$\varepsilon$でランダムな行動を選択し，確率$1-\varepsilon$でgreedy化した行動を選択する．
固定値$\alpha$方式 $Q$値の更新を式$(4)$で行っても良いが，これに疑問を抱く． $$ \begin{align} Q(s,a) = \frac{G_1 + G_2 + \cdots + G_N}{N} \end{align} $$ 方策の評価と改善(制御)を交互に行うことで，その方策は段々と最適なものに近づいていくと考えられる．例えば，初期の頃ある行動は$G_1=-4$と評価されていたが，方策の改善が進むに連れ，$G_{10}=16,G_{11}=15$と実はもっと良い行動であったとわかったとする．このときに式$(4)$で$Q$値を更新すると，$G_1,G_2,\cdots G_N$の値は全て平等に扱われる．より良い方策によって求まった$G_{10}$や$G_{11}$の方が重要な情報であるのだから，最新の$G$であればあるほど重みをつけて$Q$値を更新すべきである． 最新の情報に重みを置いた$Q$値の計算方法を式$(5)$に示す．$0\leq \alpha \leq 1$である．式 $(6)$は式$(5)$と等価である． $$ \begin{align} Q(s,a) &amp;amp;\leftarrow Q(s,a) + \alpha(G - Q(s,a)) \\ Q(s,a) &amp;amp;\leftarrow (1-\alpha)Q(s,a) + \alpha G \end{align} $$ これだけだと何が起こっているのかわかりにくいので，$Q_k$を$k$回目の$Q$値の値として$G_1,G_2,G_3,\cdots$の値で更新していく様子を見る．式$(6)$を使う． $$ \begin{align} Q_0 &amp;amp;= 0 \\ Q_1 &amp;amp;= (1-\alpha)Q_0 + \alpha G_1 = \alpha G_1 \\ Q_2 &amp;amp;= (1-\alpha)Q_1 + \alpha G_2 = (1-\alpha)\alpha G_1 + \alpha G_2 \\ Q_3 &amp;amp;= (1-\alpha)Q_2 + \alpha G_3 = (1-\alpha)^2\alpha G_1 + (1-\alpha)\alpha G_2 + \alpha G_3 \\ Q_k &amp;amp;= \sum_{i=0}^{k-1}(1-\alpha)^i\alpha G_{k-i} \end{align} $$ このように，$Q_k$の更新が終わったとき，"><title>強化学習勉強メモ #4 モンテカルロ法</title><link rel=canonical href=https://shibaken28.github.io/my-blog-4/contents/rl-4/><link rel=stylesheet href=/my-blog-4/scss/style.min.abbd69b2908fdfcd5179898beaafd374514a86538d81639ddd2c58c06ae54e40.css><meta property="og:title" content="強化学習勉強メモ #4 モンテカルロ法"><meta property="og:description" content="モンテカルロ法の説明 動的計画法の問題点 動的計画法は，状態遷移確率($p$)と報酬関数($r$)が既知である必要があった．他にも，エピソードの途中の状態を再現しなければならない点も厄介であった．
モンテカルロ法のアイデア モンテカルロ法のアイデアは，全ての行動を試すのが無理ならば，いくつかサンプリングを行って，それの平均値を取ることで，期待値を近似しようというものである1. 例えば，$10$個のサイコロを振ったときの出る目の合計の期待値を解析的に求めるには，$6^{10}=60466176$通りの計算をしなければならない．モンテカルロ法によれば，実際に$10$個のサイコロを振る動作を適当な回数(例えば$1000$回)くらい繰り返して，出た目の平均を取れば十分近似できる．
余談ではあるが，モンテカルロ法は強化学習の分野以外でも使われる．例えば，平面上にランダムな点をたくさん打って，円の中に入っている点の数を数えることで円周率を近似する手法2も，モンテカルロ法と呼ばれている．
状態価値関数をモンテカルロ法で求める 状態$s$における価値$v(s)$を求めたい場合は，状態$s$から始まるエピソードをたくさん生成し，その平均を取れば良い．これはエピソードタスクでのみ可能で，連続タスクには適用できない． エピソード$1$の収益が$G_1$，エピソード$2$の収益が$G_2$，$\cdots$，エピソード$N$の収益が$G_N$とすると，状態$s$における価値$v(s)$は式$(1)$のようになる．
$$ \begin{align} v(s) = \frac{G_1 + G_2 + \cdots + G_N}{N} \end{align} $$
最適方策を見つける 動的計画法と同じく，状態価値関数$V$から方策を改善する方策制御のターンが必要である． 動的計画法と同じく，greedy化を行って方策制御を行うことができる．箇条書きにすると次のようになる．
初期の適当な方策$\pi$を決める 十分な回数次を繰り返す． モンテカルロ法によりエピソードを$1$回通す greedy化を行い$\pi$を更新する(方策制御) ただし，このままだといくつか問題点があるため，次の手法で改善する．
行動価値関数を求める 状態価値関数$v(s)$を求めていたが，これを行動価値関数$q$に変更する．状態価値関数から最適方策を求めようとすると，式$(2)$のようになる． $$ \begin{align} \mu(s) &amp;amp;= \text{argmax} _{a} \sum _{s&amp;rsquo;} p(s&amp;rsquo;|s,a) \left\{ r(s,a,s&amp;rsquo;) + \gamma v (s&amp;rsquo;) \right\} \end{align} $$ これには，$v$に加えて，$p,r$の情報が必要になってしまう．$p,r$を直接求めることが困難な場合，最適方策がわからない．そもそも，$p,r$を直接求めることが困難であったから，ランダムにエピソードを走らせるモンテカルロ法をしているのであり，ここで$p,r$が必要になってしまったら本末転倒である．
代わりに$q$関数を使うとどうだろうか．式$(3)$のように$p$も$r$も必要なく非常にシンプルである！ $$ \begin{align} \mu(s) &amp;amp;= \text{argmax} _{a} q (s,a) \end{align} $$
求めるものが$v$から$q$へと変わったが，モンテカルロ法の適用のさせ方はほとんど変わらず，始めの状態$s$に加えて最初の行動$a$を記憶しておくだけである．
$\varepsilon$-greedy法 (方策評価フェーズ) モンテカルロ法で方策の評価をした後，単にgreedy化し，それを使ってまたモンテカルロ法を行うのは危険である． なぜなら，モンテカルロ法はあくまでサンプリングをしているのであり，収益が悪いサンプルを引いてしまうとgreedy化しても悪い方策が出てくるだけである．そこで，greedy化された方策に加え，たまにランダムな動きをするように修正を加えると，微妙な局所解に陥ってしまうことを逃れられる． 具体的には，確率$\varepsilon$でランダムな行動を選択し，確率$1-\varepsilon$でgreedy化した行動を選択する．
固定値$\alpha$方式 $Q$値の更新を式$(4)$で行っても良いが，これに疑問を抱く． $$ \begin{align} Q(s,a) = \frac{G_1 + G_2 + \cdots + G_N}{N} \end{align} $$ 方策の評価と改善(制御)を交互に行うことで，その方策は段々と最適なものに近づいていくと考えられる．例えば，初期の頃ある行動は$G_1=-4$と評価されていたが，方策の改善が進むに連れ，$G_{10}=16,G_{11}=15$と実はもっと良い行動であったとわかったとする．このときに式$(4)$で$Q$値を更新すると，$G_1,G_2,\cdots G_N$の値は全て平等に扱われる．より良い方策によって求まった$G_{10}$や$G_{11}$の方が重要な情報であるのだから，最新の$G$であればあるほど重みをつけて$Q$値を更新すべきである． 最新の情報に重みを置いた$Q$値の計算方法を式$(5)$に示す．$0\leq \alpha \leq 1$である．式 $(6)$は式$(5)$と等価である． $$ \begin{align} Q(s,a) &amp;amp;\leftarrow Q(s,a) + \alpha(G - Q(s,a)) \\ Q(s,a) &amp;amp;\leftarrow (1-\alpha)Q(s,a) + \alpha G \end{align} $$ これだけだと何が起こっているのかわかりにくいので，$Q_k$を$k$回目の$Q$値の値として$G_1,G_2,G_3,\cdots$の値で更新していく様子を見る．式$(6)$を使う． $$ \begin{align} Q_0 &amp;amp;= 0 \\ Q_1 &amp;amp;= (1-\alpha)Q_0 + \alpha G_1 = \alpha G_1 \\ Q_2 &amp;amp;= (1-\alpha)Q_1 + \alpha G_2 = (1-\alpha)\alpha G_1 + \alpha G_2 \\ Q_3 &amp;amp;= (1-\alpha)Q_2 + \alpha G_3 = (1-\alpha)^2\alpha G_1 + (1-\alpha)\alpha G_2 + \alpha G_3 \\ Q_k &amp;amp;= \sum_{i=0}^{k-1}(1-\alpha)^i\alpha G_{k-i} \end{align} $$ このように，$Q_k$の更新が終わったとき，"><meta property="og:url" content="https://shibaken28.github.io/my-blog-4/contents/rl-4/"><meta property="og:site_name" content="shibak3n's blog"><meta property="og:type" content="article"><meta property="article:section" content="Post"><meta property="article:published_time" content="2023-01-07T13:00:36+09:00"><meta property="article:modified_time" content="2023-01-07T13:00:36+09:00"><meta name=twitter:title content="強化学習勉強メモ #4 モンテカルロ法"><meta name=twitter:description content="モンテカルロ法の説明 動的計画法の問題点 動的計画法は，状態遷移確率($p$)と報酬関数($r$)が既知である必要があった．他にも，エピソードの途中の状態を再現しなければならない点も厄介であった．
モンテカルロ法のアイデア モンテカルロ法のアイデアは，全ての行動を試すのが無理ならば，いくつかサンプリングを行って，それの平均値を取ることで，期待値を近似しようというものである1. 例えば，$10$個のサイコロを振ったときの出る目の合計の期待値を解析的に求めるには，$6^{10}=60466176$通りの計算をしなければならない．モンテカルロ法によれば，実際に$10$個のサイコロを振る動作を適当な回数(例えば$1000$回)くらい繰り返して，出た目の平均を取れば十分近似できる．
余談ではあるが，モンテカルロ法は強化学習の分野以外でも使われる．例えば，平面上にランダムな点をたくさん打って，円の中に入っている点の数を数えることで円周率を近似する手法2も，モンテカルロ法と呼ばれている．
状態価値関数をモンテカルロ法で求める 状態$s$における価値$v(s)$を求めたい場合は，状態$s$から始まるエピソードをたくさん生成し，その平均を取れば良い．これはエピソードタスクでのみ可能で，連続タスクには適用できない． エピソード$1$の収益が$G_1$，エピソード$2$の収益が$G_2$，$\cdots$，エピソード$N$の収益が$G_N$とすると，状態$s$における価値$v(s)$は式$(1)$のようになる．
$$ \begin{align} v(s) = \frac{G_1 + G_2 + \cdots + G_N}{N} \end{align} $$
最適方策を見つける 動的計画法と同じく，状態価値関数$V$から方策を改善する方策制御のターンが必要である． 動的計画法と同じく，greedy化を行って方策制御を行うことができる．箇条書きにすると次のようになる．
初期の適当な方策$\pi$を決める 十分な回数次を繰り返す． モンテカルロ法によりエピソードを$1$回通す greedy化を行い$\pi$を更新する(方策制御) ただし，このままだといくつか問題点があるため，次の手法で改善する．
行動価値関数を求める 状態価値関数$v(s)$を求めていたが，これを行動価値関数$q$に変更する．状態価値関数から最適方策を求めようとすると，式$(2)$のようになる． $$ \begin{align} \mu(s) &amp;amp;= \text{argmax} _{a} \sum _{s&amp;rsquo;} p(s&amp;rsquo;|s,a) \left\{ r(s,a,s&amp;rsquo;) + \gamma v (s&amp;rsquo;) \right\} \end{align} $$ これには，$v$に加えて，$p,r$の情報が必要になってしまう．$p,r$を直接求めることが困難な場合，最適方策がわからない．そもそも，$p,r$を直接求めることが困難であったから，ランダムにエピソードを走らせるモンテカルロ法をしているのであり，ここで$p,r$が必要になってしまったら本末転倒である．
代わりに$q$関数を使うとどうだろうか．式$(3)$のように$p$も$r$も必要なく非常にシンプルである！ $$ \begin{align} \mu(s) &amp;amp;= \text{argmax} _{a} q (s,a) \end{align} $$
求めるものが$v$から$q$へと変わったが，モンテカルロ法の適用のさせ方はほとんど変わらず，始めの状態$s$に加えて最初の行動$a$を記憶しておくだけである．
$\varepsilon$-greedy法 (方策評価フェーズ) モンテカルロ法で方策の評価をした後，単にgreedy化し，それを使ってまたモンテカルロ法を行うのは危険である． なぜなら，モンテカルロ法はあくまでサンプリングをしているのであり，収益が悪いサンプルを引いてしまうとgreedy化しても悪い方策が出てくるだけである．そこで，greedy化された方策に加え，たまにランダムな動きをするように修正を加えると，微妙な局所解に陥ってしまうことを逃れられる． 具体的には，確率$\varepsilon$でランダムな行動を選択し，確率$1-\varepsilon$でgreedy化した行動を選択する．
固定値$\alpha$方式 $Q$値の更新を式$(4)$で行っても良いが，これに疑問を抱く． $$ \begin{align} Q(s,a) = \frac{G_1 + G_2 + \cdots + G_N}{N} \end{align} $$ 方策の評価と改善(制御)を交互に行うことで，その方策は段々と最適なものに近づいていくと考えられる．例えば，初期の頃ある行動は$G_1=-4$と評価されていたが，方策の改善が進むに連れ，$G_{10}=16,G_{11}=15$と実はもっと良い行動であったとわかったとする．このときに式$(4)$で$Q$値を更新すると，$G_1,G_2,\cdots G_N$の値は全て平等に扱われる．より良い方策によって求まった$G_{10}$や$G_{11}$の方が重要な情報であるのだから，最新の$G$であればあるほど重みをつけて$Q$値を更新すべきである． 最新の情報に重みを置いた$Q$値の計算方法を式$(5)$に示す．$0\leq \alpha \leq 1$である．式 $(6)$は式$(5)$と等価である． $$ \begin{align} Q(s,a) &amp;amp;\leftarrow Q(s,a) + \alpha(G - Q(s,a)) \\ Q(s,a) &amp;amp;\leftarrow (1-\alpha)Q(s,a) + \alpha G \end{align} $$ これだけだと何が起こっているのかわかりにくいので，$Q_k$を$k$回目の$Q$値の値として$G_1,G_2,G_3,\cdots$の値で更新していく様子を見る．式$(6)$を使う． $$ \begin{align} Q_0 &amp;amp;= 0 \\ Q_1 &amp;amp;= (1-\alpha)Q_0 + \alpha G_1 = \alpha G_1 \\ Q_2 &amp;amp;= (1-\alpha)Q_1 + \alpha G_2 = (1-\alpha)\alpha G_1 + \alpha G_2 \\ Q_3 &amp;amp;= (1-\alpha)Q_2 + \alpha G_3 = (1-\alpha)^2\alpha G_1 + (1-\alpha)\alpha G_2 + \alpha G_3 \\ Q_k &amp;amp;= \sum_{i=0}^{k-1}(1-\alpha)^i\alpha G_{k-i} \end{align} $$ このように，$Q_k$の更新が終わったとき，"><script async src="https://www.googletagmanager.com/gtag/js?id=G-KXVWMJ76L6"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KXVWMJ76L6")</script><link href="https://fonts.googleapis.com/css?family=Noto+Sans+JP" rel=stylesheet><style>:root{--ja-font-family:"Noto Sans JP",'ヒラギノ角ゴ Pro W3', 'Hiragino Kaku Gothic Pro', '游ゴシック', 'Yu Gothic', 'ＭＳ Ｐゴシック', 'MS PGothic', sans-serif;--base-font-family:"Lato", var(--sys-font-family), var(--ja-font-family), sans-serif;--article-font-size:12pt}.article-content .katex-display>.katex{overflow-x:hidden;overflow-y:hidden}</style></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label=メニューを開く・閉じる>
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><div class=site-meta><h1 class=site-name><a href=/my-blog-4>shibak3n's blog</a></h1><h2 class=site-description>趣味全般</h2></div></header><ol class=social-menu><li><a href=https://github.com/Shibaken28 target=_blank title=GitHub rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=https://steamcommunity.com/profiles/76561199155194438/ target=_blank title=steam rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg></a></li><li><a href=https://twitter.com/Shibak33333333n target=_blank title=Twitter rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-twitter" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M22 4.01c-1 .49-1.98.689-3 .99-1.121-1.265-2.783-1.335-4.38-.737S11.977 6.323 12 8v1c-3.245.083-6.135-1.395-8-4 0 0-4.182 7.433 4 11-1.872 1.247-3.739 2.088-6 2 3.308 1.803 6.913 2.423 10.034 1.517 3.58-1.04 6.522-3.723 7.651-7.742a13.84 13.84.0 00.497-3.753C20.18 7.773 21.692 5.25 22 4.009z"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/my-blog-4/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg><span>Home</span></a></li><li><a href=/my-blog-4/archives/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg><span>Archives</span></a></li><li><a href=/my-blog-4/search/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg><span>Search</span></a></li><li><a href=/my-blog-4/links/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg><span>Links</span></a></li><div class=menu-bottom-section><li id=dark-mode-toggle><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><span>ダークモード</span></li></div></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">目次</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#モンテカルロ法の説明>モンテカルロ法の説明</a><ol><li><a href=#動的計画法の問題点>動的計画法の問題点</a></li><li><a href=#モンテカルロ法のアイデア>モンテカルロ法のアイデア</a></li><li><a href=#状態価値関数をモンテカルロ法で求める>状態価値関数をモンテカルロ法で求める</a></li><li><a href=#最適方策を見つける>最適方策を見つける</a><ol><li><a href=#行動価値関数を求める>行動価値関数を求める</a></li><li><a href=#varepsilon-greedy法-方策評価フェーズ>$\varepsilon$-greedy法 (方策評価フェーズ)</a></li><li><a href=#固定値alpha方式>固定値$\alpha$方式</a></li></ol></li></ol></li><li><a href=#方策の種類>方策の種類</a><ol><li><a href=#重点サンプリング>重点サンプリング</a></li><li><a href=#方策オフ型のモンテカルロ法>方策オフ型のモンテカルロ法</a></li></ol></li><li><a href=#三目並べの実装>三目並べの実装</a><ol><li><a href=#雛形>雛形</a></li><li><a href=#収益の計算>収益の計算</a></li><li><a href=#最適方策>最適方策</a></li><li><a href=#結果>結果</a></li></ol></li></ol></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/my-blog-4/categories/%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92/>強化学習</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/my-blog-4/contents/rl-4/>強化学習勉強メモ #4 モンテカルロ法</a></h2></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg><time class=article-time--published>Jan 07, 2023</time></div></footer></div></header><section class=article-content><h2 id=モンテカルロ法の説明>モンテカルロ法の説明</h2><h3 id=動的計画法の問題点>動的計画法の問題点</h3><p>　動的計画法は，状態遷移確率($p$)と報酬関数($r$)が既知である必要があった．他にも，エピソードの途中の状態を再現しなければならない点も厄介であった．</p><h3 id=モンテカルロ法のアイデア>モンテカルロ法のアイデア</h3><p>　モンテカルロ法のアイデアは，全ての行動を試すのが無理ならば，いくつかサンプリングを行って，それの平均値を取ることで，期待値を近似しようというものである<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>.
例えば，$10$個のサイコロを振ったときの出る目の合計の期待値を解析的に求めるには，$6^{10}=60466176$通りの計算をしなければならない．モンテカルロ法によれば，実際に$10$個のサイコロを振る動作を適当な回数(例えば$1000$回)くらい繰り返して，出た目の平均を取れば十分近似できる．</p><p>　余談ではあるが，モンテカルロ法は強化学習の分野以外でも使われる．例えば，平面上にランダムな点をたくさん打って，円の中に入っている点の数を数えることで円周率を近似する手法<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>も，モンテカルロ法と呼ばれている．</p><h3 id=状態価値関数をモンテカルロ法で求める>状態価値関数をモンテカルロ法で求める</h3><p>　状態$s$における価値$v(s)$を求めたい場合は，状態$s$から始まるエピソードをたくさん生成し，その平均を取れば良い．これはエピソードタスクでのみ可能で，連続タスクには適用できない．
エピソード$1$の収益が$G_1$，エピソード$2$の収益が$G_2$，$\cdots$，エピソード$N$の収益が$G_N$とすると，状態$s$における価値$v(s)$は式$(1)$のようになる．</p><p>$$
\begin{align}
v(s) = \frac{G_1 + G_2 + \cdots + G_N}{N}
\end{align}
$$</p><h3 id=最適方策を見つける>最適方策を見つける</h3><p>　動的計画法と同じく，状態価値関数$V$から方策を改善する<strong>方策制御</strong>のターンが必要である．
動的計画法と同じく，greedy化を行って方策制御を行うことができる．箇条書きにすると次のようになる．</p><ol><li>初期の適当な方策$\pi$を決める</li><li>十分な回数次を繰り返す．<ol><li>モンテカルロ法によりエピソードを$1$回通す</li><li>greedy化を行い$\pi$を更新する(方策制御)</li></ol></li></ol><p>ただし，このままだといくつか問題点があるため，次の手法で改善する．</p><h4 id=行動価値関数を求める>行動価値関数を求める</h4><p>　状態価値関数$v(s)$を求めていたが，これを行動価値関数$q$に変更する．状態価値関数から最適方策を求めようとすると，式$(2)$のようになる．
$$
\begin{align}
\mu(s) &= \text{argmax} _{a} \sum _{s&rsquo;} p(s&rsquo;|s,a) \left\{ r(s,a,s&rsquo;) + \gamma v (s&rsquo;) \right\}
\end{align}
$$
これには，$v$に加えて，$p,r$の情報が必要になってしまう．$p,r$を直接求めることが困難な場合，最適方策がわからない．そもそも，$p,r$を直接求めることが困難であったから，ランダムにエピソードを走らせるモンテカルロ法をしているのであり，ここで$p,r$が必要になってしまったら本末転倒である．</p><p>　代わりに$q$関数を使うとどうだろうか．式$(3)$のように$p$も$r$も必要なく非常にシンプルである！
$$
\begin{align}
\mu(s) &= \text{argmax} _{a} q (s,a)
\end{align}
$$</p><p>求めるものが$v$から$q$へと変わったが，モンテカルロ法の適用のさせ方はほとんど変わらず，始めの状態$s$に加えて最初の行動$a$を記憶しておくだけである．</p><h4 id=varepsilon-greedy法-方策評価フェーズ>$\varepsilon$-greedy法 (方策評価フェーズ)</h4><p>　モンテカルロ法で方策の評価をした後，単にgreedy化し，それを使ってまたモンテカルロ法を行うのは危険である．
なぜなら，モンテカルロ法はあくまでサンプリングをしているのであり，収益が悪いサンプルを引いてしまうとgreedy化しても悪い方策が出てくるだけである．そこで，greedy化された方策に加え，たまにランダムな動きをするように修正を加えると，微妙な局所解に陥ってしまうことを逃れられる．
具体的には，確率$\varepsilon$でランダムな行動を選択し，確率$1-\varepsilon$でgreedy化した行動を選択する．</p><h4 id=固定値alpha方式>固定値$\alpha$方式</h4><p>　$Q$値の更新を式$(4)$で行っても良いが，これに疑問を抱く．
$$
\begin{align}
Q(s,a) = \frac{G_1 + G_2 + \cdots + G_N}{N}
\end{align}
$$
方策の評価と改善(制御)を交互に行うことで，その方策は段々と最適なものに近づいていくと考えられる．例えば，初期の頃ある行動は$G_1=-4$と評価されていたが，方策の改善が進むに連れ，$G_{10}=16,G_{11}=15$と実はもっと良い行動であったとわかったとする．このときに式$(4)$で$Q$値を更新すると，$G_1,G_2,\cdots G_N$の値は全て平等に扱われる．より良い方策によって求まった$G_{10}$や$G_{11}$の方が重要な情報であるのだから，最新の$G$であればあるほど重みをつけて$Q$値を更新すべきである．
最新の情報に重みを置いた$Q$値の計算方法を式$(5)$に示す．$0\leq \alpha \leq 1$である．式
$(6)$は式$(5)$と等価である．
$$
\begin{align}
Q(s,a) &\leftarrow Q(s,a) + \alpha(G - Q(s,a)) \\
Q(s,a) &\leftarrow (1-\alpha)Q(s,a) + \alpha G
\end{align}
$$
これだけだと何が起こっているのかわかりにくいので，$Q_k$を$k$回目の$Q$値の値として$G_1,G_2,G_3,\cdots$の値で更新していく様子を見る．式$(6)$を使う．
$$
\begin{align}
Q_0 &= 0 \\
Q_1 &= (1-\alpha)Q_0 + \alpha G_1 = \alpha G_1 \\
Q_2 &= (1-\alpha)Q_1 + \alpha G_2 = (1-\alpha)\alpha G_1 + \alpha G_2 \\
Q_3 &= (1-\alpha)Q_2 + \alpha G_3 = (1-\alpha)^2\alpha G_1 + (1-\alpha)\alpha G_2 + \alpha G_3 \\
Q_k &= \sum_{i=0}^{k-1}(1-\alpha)^i\alpha G_{k-i}
\end{align}
$$
このように，$Q_k$の更新が終わったとき，</p><ul><li>$G_k$の重みは$\alpha$</li><li>$G_{k-1}$の重みは$(1-\alpha)\alpha$</li><li>$G_{k-2}$の重みは$(1-\alpha)^2\alpha$</li><li>$\cdots$</li></ul><p>と，古い情報であればあるほど重みが小さくなる．
このように，<strong>新しい情報に重みをおきたいときに固定値$\alpha$方式</strong>は有効である．</p><h2 id=方策の種類>方策の種類</h2><p>　これまで方策と呼んできたものは，次のように分類することができる．</p><ul><li>いまから改善していく方策 = <strong>ターゲット方策</strong></li><li>エージェントが実際に行動するときの方策 = <strong>挙動方策</strong></li></ul><p>一つの方策がターゲット方策と挙動方策の両方の役割を持つ場合，それは<strong>方策オン型</strong>のアプローチと呼ぶ．逆に，別々の方策の場合は<strong>方策オフ型</strong>と呼ぶ．前回のモンテカルロ法は，始めに決めた方策に従ってエピソードを走らせ，その方策を改善し，改善された方策でまたエピソードを走る，というように一つの方策で行っているため，方策オン型である．</p><p>　では，方策オフ型のモンテカルロ法があったとすると，それは，挙動方策が探索を行い，ターゲット方策にはそれを活用させるようなものである．このとき，挙動方策から得られたサンプリングデータ$x$は確率分布$b(x)$に従うが，実際のターゲット方策は確率分布$\pi(x)$に従うという状況になる．このとき，$\pi$における$x$の期待値$\mathbb{E}_\pi[x]$を求めるには少し工夫が必要である．</p><h3 id=重点サンプリング>重点サンプリング</h3><p>　この問題を数学的に表現すると，次のようになる．</p><blockquote><p>ある確率分布$\pi(x)$の期待値$\mathbb{E}_\pi[x]$を，別の確率分布$b(x)$からのサンプリングで得られたデータでどう表現するか．</p></blockquote><p>次の式変形を行う．
$$
\begin{align}
\mathbb{E}_\pi &= \sum x\pi(x) \nonumber \\
&= \sum x\pi(x) \frac{b(x)}{b(x)} \nonumber \\
&= \sum x\frac{\pi(x)}{b(x)} b(x) \nonumber \\
&= \mathbb{E}_b \left[x \frac{\pi(x)}{b(x)} \right] \nonumber
\end{align}
$$
これにより，$\pi(x)$の期待値は，$x$に$\pi(x)$と$b(x)$の比をかけたものの期待値と等しいことがわかる．これだけだとありがたみが分かりづらいので具体例としてモンテカルロ法を説明する．</p><h3 id=方策オフ型のモンテカルロ法>方策オフ型のモンテカルロ法</h3><p>　重点サンプリングを用いることで，ある行動をする確率分布を，ターゲット方策をgreedyな方策の$\pi$，
挙動方策を$\varepsilon$-greedyな方策の$b$に設定する，という方策オフ型のアルゴリズムの実現が可能になる．
まず，簡単な例として，次のように時刻$S_t$から行動$A_t$を取り，報酬$R_t$を得て，状態$S_t$で終了したとする．これを$\mathrm{trajectory}$と呼ぶことにする．
$$
\mathrm{trajectory} = S_{t} \rightarrow A_{t} \rightarrow R_{t} \rightarrow S_{t+1}
$$
これが方策$\pi$で行った結果であるのなら収益$G_t$が得られ，これがたくさん得られれば，それらの平均を取り，$\mathbb{E}[G_t]$の計算にそのまま使用できる．一方，方策$b$で行った場合であればそうはいかない．
方策$\pi$で行った場合，この$\mathrm{trajectory}$を取る確率は次のようになる．
$\mathrm{Pr}(\mathrm{trajectory}|\pi)$は方策$\pi$のときに一連の流れ$\mathrm{trajectory}$が起こる確率を表す．
$$
\mathrm{Pr}(\mathrm{trajectory}|\pi) = p(S_{t+1}|S_{t},A_{t})\pi(A_{t}|S_{t})
$$
一方，方策$b$で行った場合，この$\mathrm{trajectory}$を取る確率は次のようになる．
$$
\mathrm{Pr}(\mathrm{trajectory}|b) = p(S_{t+1}|S_{t},A_{t})b(A_{t}|S_{t})
$$</p><p>よって，$G_t$の期待値は次のようになる．
$$
\mathbb{E}_{\pi}[G_{t}] = \mathbb{E}_{b} \left[ G_{t} \frac{\mathrm{Pr}(\mathrm{trajectory}|\pi)}{\mathrm{Pr}(\mathrm{trajectory}|b)} \right]
= \mathbb{E}_{b} \left[ G_{t} \frac{\pi(A_{t}|S_{t})}{b(A_{t}|S_{t})} \right]
$$</p><p>一般化する．時刻$t$から始まったエピソードは時刻$T$の状態$S_T$で終了したとする．
$$
\mathrm{trajectory} = S_{t} \rightarrow A_{t} \rightarrow R_{t} \rightarrow S_{t+1} \rightarrow A_{t+1} \rightarrow R_{t+1} \rightarrow \cdots \rightarrow S_{T-1} \rightarrow A_{T-1} \rightarrow R_{T-1} \rightarrow S_{T}
$$
先ほどと同様に，各方策で$\mathrm{trajectory}$を取る確率は次のようになる．
$$
\mathrm{Pr}(\mathrm{trajectory}|\pi) = \prod_{\tau=t}^{T-1} p(S_{\tau+1}|S_{\tau},A_{\tau})\pi(A_{\tau}|S_{\tau}) \\
\mathrm{Pr}(\mathrm{trajectory}|b) = \prod_{\tau=t}^{T-1} p(S_{\tau+1}|S_{\tau},A_{\tau})b(A_{\tau}|S_{\tau})
$$
よって，得られた$G_t$に
$$
\frac{\mathrm{Pr}(\mathrm{trajectory}|\pi)}{\mathrm{Pr}(\mathrm{trajectory}|b)} = \prod_{\tau=t}^{T-1} \frac{\pi(A_{\tau}|S_{\tau})}{b(A_{\tau}|S_{\tau})}
$$
を掛け算し，それらの平均を取れば方策$\pi$の期待値に近似できる．</p><h2 id=三目並べの実装>三目並べの実装</h2><p>　$Q$値を$\varepsilon$-greedy法と固定値$\alpha$方式を使いながら求めるプログラムを本当は書きたかったのだが，うまくいかなかった(うまくいかない理由がよくわかっていないのでいつかやりたい)．</p><p>完成版のソースコードは<a class=link href=https://gist.github.com/Shibaken28/db7f1f7c0e992bfdfab7d8c52d5bf5b8 target=_blank rel=noopener>こちら</a>．
　
ここから下にあるのは方策制御を行わないモンテカルロ法で，三目並べを実装したものである．</p><h3 id=雛形>雛形</h3><p>　モンテカルロ法は，適当な回数だけ試行を行うため，プログラムの形は次のようになる．</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=kt>int</span> <span class=nf>main</span><span class=p>(){</span>
</span></span><span class=line><span class=cl>    <span class=c1>// コンピュータは常にoとする
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=c1>//  V[s] := 状態sにおける価値
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=n>map</span><span class=o>&lt;</span><span class=n>Board</span><span class=p>,</span> <span class=kt>double</span><span class=o>&gt;</span> <span class=n>V</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>map</span><span class=o>&lt;</span><span class=n>Board</span><span class=p>,</span> <span class=kt>int</span><span class=o>&gt;</span> <span class=n>count</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1>//モンテカルロ法
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>const</span> <span class=kt>int</span> <span class=n>N</span> <span class=o>=</span> <span class=mi>100000</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=kt>double</span> <span class=n>gamma</span> <span class=o>=</span> <span class=mf>0.9</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span><span class=p>(</span><span class=kt>int</span> <span class=n>i</span><span class=o>=</span><span class=mi>0</span><span class=p>;</span><span class=n>i</span><span class=o>&lt;</span><span class=n>N</span><span class=p>;</span><span class=n>i</span><span class=o>++</span><span class=p>){</span>
</span></span><span class=line><span class=cl>        <span class=n>Board</span> <span class=n>board</span> <span class=o>=</span> <span class=p>{</span><span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>};</span>
</span></span><span class=line><span class=cl>        <span class=c1>// エピソードを生成，収益を計算
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><p>これだと一番始めの状態からの試行しかできないのではないかと思われるかもしれないが，途中の状態における収益も，同時に計算を行うことができる．</p><h3 id=収益の計算>収益の計算</h3><p>　$G_{s}$を状態$s$から始まるエピソードの収益とする．
$s_1$の次の状態を$s_2$，その次の状態を$s_3$，その次が$s_4$で，そこでエピソードが終了したとすると，各状態$s_i$における収益$G_{s_i}$は式$(12)$から$(15)$で表される．
$$
\begin{align}
G_{s_1} =& r_{1} + \gamma r_{2} + \gamma^2 r_{3} + \gamma^3 r_{4} \\
G_{s_2} =& r_{2} + \gamma r_{3} + \gamma^2 r_{4} \\
G_{s_3} =& r_{3} + \gamma r_{4} \\
G_{s_4} =& r_{4} \\
\end{align}
$$
ナイーブな実装では，$s_1$から始まるエピソードの収益を計算するたびに，$s_2$から始まるエピソードの収益を計算しなければならないが，実は，式$(16)$のように，$G_{s_1}$の中には，$G_{s_2}$が含まれている．</p><p>$$
\begin{align}
G_{s_1} = r_{1} + \gamma G_{s_2}
\end{align}
$$</p><p>同様に，$G_{s_2}$の中には，$G_{s_3}$が含まれ，$G_{s_3}$の中には，$G_{s_4}$が含まれている．よって，エピソードが終了したときに，各報酬と各状態を逆順にたどっていけば，各状態における収益を計算することができる．</p><p>$$
\begin{align}
G_{k} &= r_{k}\\
G_{s_{k-1}} &= r_{k-1} + \gamma G_{s_{k}} \\
\cdots \nonumber \\
G_{s_2} &= r_{2} + \gamma G_{s_3} \\
G_{s_1} &= r_{1} + \gamma G_{s_2}
\end{align}
$$</p><p>さらに，今回はエピソードが終了するときにしか報酬が発生しないので，式$(21)$から$(24)$の通り，等比数列のように計算できる．
$$
\begin{align}
G_{s_1} = \gamma^0 r \\
G_{s_2} = \gamma^1 r \\
G_{s_3} = \gamma^2 r \\
\cdots \nonumber \\
G_{s_{k}} = \gamma^{k-1} r
\end{align}
$$</p><p>これに基づき，次のように実装する．</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl>    <span class=n>map</span><span class=o>&lt;</span><span class=n>Board</span><span class=p>,</span> <span class=kt>double</span><span class=o>&gt;</span> <span class=n>V</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>map</span><span class=o>&lt;</span><span class=n>Board</span><span class=p>,</span> <span class=kt>int</span><span class=o>&gt;</span> <span class=n>count</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>const</span> <span class=kt>int</span> <span class=n>N</span> <span class=o>=</span> <span class=mi>100000</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=kt>double</span> <span class=n>gamma</span> <span class=o>=</span> <span class=mf>0.9</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>N</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=n>Board</span> <span class=n>board</span> <span class=o>=</span> <span class=p>{</span><span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>};</span>
</span></span><span class=line><span class=cl>        <span class=n>vector</span><span class=o>&lt;</span><span class=n>Board</span><span class=o>&gt;</span> <span class=n>history</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=kt>int</span> <span class=n>turn</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=k>while</span> <span class=p>(</span><span class=nb>true</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=n>history</span><span class=p>.</span><span class=n>push_back</span><span class=p>(</span><span class=n>board</span><span class=p>);</span><span class=err>　</span>
</span></span><span class=line><span class=cl>            <span class=kt>int</span> <span class=n>act</span>
</span></span><span class=line><span class=cl>            <span class=k>while</span><span class=p>(</span><span class=nb>true</span><span class=p>){</span>
</span></span><span class=line><span class=cl>                <span class=n>act</span> <span class=o>=</span> <span class=n>rand</span><span class=p>()</span> <span class=o>%</span> <span class=n>cells</span><span class=p>;</span>
</span></span><span class=line><span class=cl>                <span class=k>if</span> <span class=p>(</span><span class=n>board</span><span class=p>[</span><span class=n>act</span><span class=p>]</span> <span class=o>==</span> <span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                    <span class=k>break</span><span class=p>;</span>
</span></span><span class=line><span class=cl>            <span class=p>}</span>
</span></span><span class=line><span class=cl>            <span class=n>board</span><span class=p>[</span><span class=n>act</span><span class=p>]</span> <span class=o>=</span> <span class=n>turn</span> <span class=o>+</span> <span class=mi>1</span><span class=p>;</span> <span class=c1>//自分の記号を置く
</span></span></span><span class=line><span class=cl><span class=c1></span>            <span class=k>if</span> <span class=p>(</span><span class=n>judge</span><span class=p>(</span><span class=n>board</span><span class=p>)</span> <span class=o>!=</span> <span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=k>break</span><span class=p>;</span>
</span></span><span class=line><span class=cl>            <span class=n>turn</span> <span class=o>=</span> <span class=mi>1</span> <span class=o>-</span> <span class=n>turn</span><span class=p>;</span> <span class=c1>//次の手番
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=n>history</span><span class=p>.</span><span class=n>push_back</span><span class=p>(</span><span class=n>board</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=kt>double</span> <span class=n>r</span> <span class=o>=</span> <span class=n>reward</span><span class=p>(</span><span class=n>board</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=n>reverse</span><span class=p>(</span><span class=n>history</span><span class=p>.</span><span class=n>begin</span><span class=p>(),</span> <span class=n>history</span><span class=p>.</span><span class=n>end</span><span class=p>());</span> <span class=c1>//逆順にする
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=k>for</span> <span class=p>(</span><span class=k>const</span> <span class=k>auto</span> <span class=o>&amp;</span><span class=nl>h</span> <span class=p>:</span> <span class=n>history</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=n>V</span><span class=p>[</span><span class=n>h</span><span class=p>]</span> <span class=o>+=</span> <span class=n>r</span><span class=p>;</span>
</span></span><span class=line><span class=cl>            <span class=n>count</span><span class=p>[</span><span class=n>h</span><span class=p>]</span><span class=o>++</span><span class=p>;</span>
</span></span><span class=line><span class=cl>            <span class=n>r</span> <span class=o>*=</span> <span class=n>gamma</span><span class=p>;</span> <span class=c1>//割引率をかける
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=k>auto</span> <span class=o>&amp;</span><span class=p>[</span><span class=n>prevS</span><span class=p>,</span> <span class=n>prevV</span><span class=p>]</span> <span class=o>:</span> <span class=n>V</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=n>prevV</span> <span class=o>/=</span> <span class=n>count</span><span class=p>[</span><span class=n>prevS</span><span class=p>];</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><p>価値反復法のときには相手の手番を考慮していたが，今回は簡単のために完全にランダムにする．
各状態からの試行回数をカウントし，最後に回数で割って平均をとる．</p><p>　プログラムを詳しく解説していく．
<code>while</code>文では，ランダムな空いている場所を選ぶまで繰り返している<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>．
空いている場所を列挙して，その中からランダムに選ぶ実装でも良いが，今回は実行時間に余裕があるので実装をサボった(読者への課題としよう！)．</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl>            <span class=k>while</span><span class=p>(</span><span class=nb>true</span><span class=p>){</span>
</span></span><span class=line><span class=cl>                <span class=n>act</span> <span class=o>=</span> <span class=n>rand</span><span class=p>()</span> <span class=o>%</span> <span class=n>cells</span><span class=p>;</span>
</span></span><span class=line><span class=cl>                <span class=k>if</span> <span class=p>(</span><span class=n>board</span><span class=p>[</span><span class=n>act</span><span class=p>]</span> <span class=o>==</span> <span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                    <span class=k>break</span><span class=p>;</span>
</span></span><span class=line><span class=cl>            <span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><p><code>history</code>には，盤面の情報$s_1,s_2,s_3,\cdots,s_n$が順番に記録されていく．一番最後の盤面の報酬が$r$だったとき，各盤面の収益は，後ろから$r,\gamma r,\gamma^2 r,\cdots$となる．</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl>        <span class=kt>double</span> <span class=n>r</span> <span class=o>=</span> <span class=n>reward</span><span class=p>(</span><span class=n>board</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=n>reverse</span><span class=p>(</span><span class=n>history</span><span class=p>.</span><span class=n>begin</span><span class=p>(),</span> <span class=n>history</span><span class=p>.</span><span class=n>end</span><span class=p>());</span> <span class=c1>//逆順にする
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=k>for</span> <span class=p>(</span><span class=k>const</span> <span class=k>auto</span> <span class=o>&amp;</span><span class=nl>h</span> <span class=p>:</span> <span class=n>history</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=n>V</span><span class=p>[</span><span class=n>h</span><span class=p>]</span> <span class=o>+=</span> <span class=n>r</span><span class=p>;</span>
</span></span><span class=line><span class=cl>            <span class=n>count</span><span class=p>[</span><span class=n>h</span><span class=p>]</span><span class=o>++</span><span class=p>;</span>
</span></span><span class=line><span class=cl>            <span class=n>r</span> <span class=o>*=</span> <span class=n>gamma</span><span class=p>;</span> <span class=c1>//割引率をかける
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=最適方策>最適方策</h3><p>　最も価値が高くなるような行動を選び，最適方策$\mu$を決定する．</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl>    <span class=n>map</span><span class=o>&lt;</span><span class=n>Board</span><span class=p>,</span> <span class=kt>int</span><span class=o>&gt;</span> <span class=n>mu</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=k>auto</span> <span class=o>&amp;</span><span class=p>[</span><span class=n>prevS</span><span class=p>,</span> <span class=n>prevV</span><span class=p>]</span> <span class=o>:</span> <span class=n>V</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=p>(</span><span class=o>!</span><span class=n>isVaild</span><span class=p>(</span><span class=n>prevS</span><span class=p>))</span>
</span></span><span class=line><span class=cl>            <span class=k>continue</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=kt>int</span> <span class=n>act</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=kt>double</span> <span class=n>nextV</span> <span class=o>=</span> <span class=o>-</span><span class=mf>1e9</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>cells</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=p>(</span><span class=n>prevS</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>!=</span> <span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=k>continue</span><span class=p>;</span>
</span></span><span class=line><span class=cl>            <span class=n>Board</span> <span class=n>nextS</span> <span class=o>=</span> <span class=n>prevS</span><span class=p>;</span>
</span></span><span class=line><span class=cl>            <span class=n>nextS</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>=</span> <span class=mi>1</span><span class=p>;</span>
</span></span><span class=line><span class=cl>            <span class=n>cout</span> <span class=o>&lt;&lt;</span> <span class=n>i</span> <span class=o>&lt;&lt;</span> <span class=s>&#34; : &#34;</span> <span class=o>&lt;&lt;</span> <span class=n>V</span><span class=p>[</span><span class=n>nextS</span><span class=p>]</span> <span class=o>&lt;&lt;</span> <span class=n>endl</span><span class=p>;</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=p>(</span><span class=n>V</span><span class=p>[</span><span class=n>nextS</span><span class=p>]</span> <span class=o>&gt;</span> <span class=n>nextV</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=p>{</span>
</span></span><span class=line><span class=cl>                <span class=n>nextV</span> <span class=o>=</span> <span class=n>V</span><span class=p>[</span><span class=n>nextS</span><span class=p>];</span>
</span></span><span class=line><span class=cl>                <span class=n>act</span> <span class=o>=</span> <span class=n>i</span><span class=p>;</span>
</span></span><span class=line><span class=cl>            <span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=n>mu</span><span class=p>[</span><span class=n>prevS</span><span class=p>]</span> <span class=o>=</span> <span class=n>act</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=n>cout</span> <span class=o>&lt;&lt;</span> <span class=s>&#34;mu [&#34;</span> <span class=o>&lt;&lt;</span> <span class=n>endl</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=n>print_board</span><span class=p>(</span><span class=n>prevS</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=n>cout</span> <span class=o>&lt;&lt;</span> <span class=s>&#34;] = &#34;</span> <span class=o>&lt;&lt;</span> <span class=n>mu</span><span class=p>[</span><span class=n>prevS</span><span class=p>]</span> <span class=o>&lt;&lt;</span> <span class=n>endl</span>
</span></span><span class=line><span class=cl>             <span class=o>&lt;&lt;</span> <span class=n>endl</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=結果>結果</h3><p>各状態の価値を表示すると，以下のようになる(抜粋)．</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=mi>0</span> <span class=o>:</span> <span class=o>-</span><span class=mf>0.121793</span>
</span></span><span class=line><span class=cl><span class=mi>4</span> <span class=o>:</span> <span class=o>-</span><span class=mf>0.20035</span>
</span></span><span class=line><span class=cl><span class=mi>5</span> <span class=o>:</span> <span class=o>-</span><span class=mf>0.662595</span>
</span></span><span class=line><span class=cl><span class=n>mu</span> <span class=p>[</span>
</span></span><span class=line><span class=cl> <span class=o>|</span><span class=n>x</span><span class=o>|</span><span class=n>x</span>
</span></span><span class=line><span class=cl><span class=n>o</span><span class=o>|</span> <span class=o>|</span> 
</span></span><span class=line><span class=cl><span class=n>x</span><span class=o>|</span><span class=n>o</span><span class=o>|</span><span class=n>o</span>
</span></span><span class=line><span class=cl><span class=p>]</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=mi>0</span> <span class=o>:</span> <span class=mf>0.323512</span>
</span></span><span class=line><span class=cl><span class=mi>4</span> <span class=o>:</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl><span class=mi>6</span> <span class=o>:</span> <span class=o>-</span><span class=mf>0.0316175</span>
</span></span><span class=line><span class=cl><span class=mi>7</span> <span class=o>:</span> <span class=o>-</span><span class=mf>0.0757296</span>
</span></span><span class=line><span class=cl><span class=mi>8</span> <span class=o>:</span> <span class=o>-</span><span class=mf>0.291569</span>
</span></span><span class=line><span class=cl><span class=n>mu</span> <span class=p>[</span>
</span></span><span class=line><span class=cl> <span class=o>|</span><span class=n>x</span><span class=o>|</span><span class=n>x</span>
</span></span><span class=line><span class=cl><span class=n>o</span><span class=o>|</span> <span class=o>|</span><span class=n>o</span>
</span></span><span class=line><span class=cl> <span class=o>|</span> <span class=o>|</span> 
</span></span><span class=line><span class=cl><span class=p>]</span> <span class=o>=</span> <span class=mi>4</span>
</span></span></code></pre></td></tr></table></div></div><p>対戦してみると，以下のようになる．</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl> <span class=o>|</span> <span class=o>|</span> 
</span></span><span class=line><span class=cl> <span class=o>|</span> <span class=o>|</span> 
</span></span><span class=line><span class=cl> <span class=o>|</span> <span class=o>|</span> 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl> <span class=o>|</span> <span class=o>|</span> 
</span></span><span class=line><span class=cl> <span class=o>|</span><span class=n>o</span><span class=o>|</span> 
</span></span><span class=line><span class=cl> <span class=o>|</span> <span class=o>|</span> 
</span></span><span class=line><span class=cl><span class=nl>put</span><span class=p>:</span><span class=mi>5</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl> <span class=o>|</span> <span class=o>|</span> 
</span></span><span class=line><span class=cl> <span class=o>|</span><span class=n>o</span><span class=o>|</span><span class=n>x</span>
</span></span><span class=line><span class=cl> <span class=o>|</span> <span class=o>|</span> 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl> <span class=o>|</span> <span class=o>|</span> 
</span></span><span class=line><span class=cl> <span class=o>|</span><span class=n>o</span><span class=o>|</span><span class=n>x</span>
</span></span><span class=line><span class=cl> <span class=o>|</span> <span class=o>|</span><span class=n>o</span>
</span></span><span class=line><span class=cl><span class=nl>put</span><span class=p>:</span><span class=mi>0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>x</span><span class=o>|</span> <span class=o>|</span> 
</span></span><span class=line><span class=cl> <span class=o>|</span><span class=n>o</span><span class=o>|</span><span class=n>x</span>
</span></span><span class=line><span class=cl> <span class=o>|</span> <span class=o>|</span><span class=n>o</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>x</span><span class=o>|</span> <span class=o>|</span> 
</span></span><span class=line><span class=cl> <span class=o>|</span><span class=n>o</span><span class=o>|</span><span class=n>x</span>
</span></span><span class=line><span class=cl><span class=n>o</span><span class=o>|</span> <span class=o>|</span><span class=n>o</span>
</span></span><span class=line><span class=cl><span class=nl>put</span><span class=p>:</span><span class=mi>7</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>x</span><span class=o>|</span> <span class=o>|</span> 
</span></span><span class=line><span class=cl> <span class=o>|</span><span class=n>o</span><span class=o>|</span><span class=n>x</span>
</span></span><span class=line><span class=cl><span class=n>o</span><span class=o>|</span><span class=n>x</span><span class=o>|</span><span class=n>o</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>x</span><span class=o>|</span> <span class=o>|</span><span class=n>o</span>
</span></span><span class=line><span class=cl> <span class=o>|</span><span class=n>o</span><span class=o>|</span><span class=n>x</span>
</span></span><span class=line><span class=cl><span class=n>o</span><span class=o>|</span><span class=n>x</span><span class=o>|</span><span class=n>o</span>
</span></span></code></pre></td></tr></table></div></div><p>初手は真ん中に置いてきて，こちらが辺に置くと相手は勝ってくれる．</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>大数の法則より，サンプリング数が増えると，平均値は期待値に近づいていく．&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p><a class=link href=https://manabitimes.jp/math/1182 target=_blank rel=noopener>https://manabitimes.jp/math/1182</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>最後の$1$マスは$1/9$の確率を引くまでループし続ける．マス数が多くなると当然空いているマスを選ぶまで時間がかかるので注意．&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></section><footer class=article-footer><section class=article-copyright><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg><span>Licensed under CC BY-NC-SA 4.0</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/katex.min.css integrity="sha256-J+iAE0sgH8QSz9hpcDxXIftnj65JEZgNhGcgReTTK9s=" crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/katex.min.js integrity="sha256-InsNdER1b2xUewP+pKCUJpkhiqwHgqiPXDlIk7GzBu4=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/contrib/auto-render.min.js integrity="sha256-y39Mpg7V3D4lhBX4x6O0bUqTV4pSrfgwEfGKfxkOdgI=" crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{renderMathInElement(document.querySelector(`.article-content`),{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><aside class=related-content--wrapper><h2 class=section-title>関連するコンテンツ</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/my-blog-4/contents/puyo-dqn/><div class=article-details><h2 class=article-title>強化学習でぷよぷよを学習させたかった(願望)</h2></div></a></article><article><a href=/my-blog-4/contents/rl-0/><div class=article-details><h2 class=article-title>強化学習勉強メモ (目次)</h2></div></a></article></div></div></aside><footer class=site-footer><section class=copyright>&copy;
2022 -
2024 shibak3n's blog</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>テーマ <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.21.0>Stack</a></b> は <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a> によって設計されています。</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/my-blog-4/ts/main.js defer></script>
<script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>
<!doctype html><html lang=ja dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="本記事では，主に用語の確認をする．
なお，強化学習において頻繁に登場する記号たちをExtraMaterialにまとめている．
エージェントと環境 エージェントは，強化学習における主人公的なポジションであり， 行動(Action) をする．エージェントの行動によって，変化するものが 環境(Environment) である．エージェントは，環境の 状態(State) を得ることによって，次の行動を決定する．行動によって評価されるものが 報酬(Reward) である．
まとめると，時刻$t$のとき次の手順を踏む．
エージェントは環境から状態$S_t$を得る． エージェントは状態$S_t$を用いて行動$A_t$を決定する． エージェントは行動$A_t$を実行する． エージェントは環境から報酬$R_t$を得る． 時刻$t+1$に移る． 決定的，確率的挙動 ロボット(エージェント)が迷路の中で，右に移動するという行動を取った．しかし，たまたま床がツルツルで滑ってしまうことが原因で，$0.1$の確立で左に移動してしまうとする．あるいは，エージェント以外が状態を変化させるような問題も考えられる．このような，行動に対して状態が確率によって変化することを 確率的(stochastic) な挙動 と呼ぶ．逆に，行動が確率によって変化しないことを 決定的(deterministic) な挙動 と呼ぶ．
決定的な挙動は式($1$)で表される． $$ \begin{align} s^\prime = f(s,a) \end{align} $$ $f$は状態$s$と行動$a$を受け取り，次の状態$s^\prime$を返す関数である．
また，状態$s$と行動$a$を取って状態$s^\prime$が得られる確率を式($2$)の$p$で表記する． $$ \begin{align} p(s^\prime|s,a) \end{align} $$ $p(s^\prime|s,a)$のパイプラインのような記号$|$を用いた表記は，$p(s^\prime)$の$s^\prime$に条件$s,a$を付け足すものである． 例えば，迷路の例で，
$s$=スタート地点 $a$=上に進む $s^\prime$=座標1に移動する の場合$p(s^\prime|s,a)$は「スタート地点にいて上に進んだときに，座標1に移動する確率」を表している．
なお，決定的な挙動の場合，$p(s^\prime|s,a)=1$であると考えれば確率的な挙動の特殊ケースと捉えられる．
マルコフ性 $p(s^\prime|s,a)$という式は，次の状態$s^\prime$が現在の状態$s$と行動$a$のみから決まることを意味している．このような性質を マルコフ性(Markov property) と呼ぶ． 別の言い方をすると，過去の行動や状態は考えなくて良いということである．このマルコフ性を仮定することにより，過去の行動を考えた膨大な行動パターンを考える必要がなくなり，問題が解きやすくなる．
方策 マルコフ性を仮定した場合，現在の状態$s$のみから次の行動$a$を決定する．行動の方針のことを 方策(policy) と呼ぶ． 状態$s$であるときに，行動$a$を取る確率を$\pi(a|s)$で表現する． 決定論的な場合は$a=\mu(s)$と表すこともある．
エピソードタスクと連続タスク エピソードタスクはオセロやチェスのような終わりのある問題のことである．オセロであれば，最終的には勝ち負け引き分けのいずれかになる．対して，終わりが考えにくいような問題を連続タスクと呼ぶ．注文を確認して在庫を管理するような問題は，これといった終わりを考えないエンドレスなものだと認識すると連続タスクだと考えられる．
収益 エージェントは時刻$t$から報酬$R_t,R_{t+1},R_{t+2},\cdots$を得ていく．これらの報酬をあわせたものを収益と呼ぶ． 時刻$t$の収益$G_t$は具体的に式($3$)の計算をする．ただし，$\gamma$は割引率と呼び，$0\leq \gamma \leq 1$である．
$$ \begin{align} G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \cdots \end{align} $$"><title>強化学習勉強メモ #1 用語確認</title><link rel=canonical href=https://shibaken28.github.io/my-blog-4/contents/rl-1/><link rel=stylesheet href=/my-blog-4/scss/style.min.abbd69b2908fdfcd5179898beaafd374514a86538d81639ddd2c58c06ae54e40.css><meta property="og:title" content="強化学習勉強メモ #1 用語確認"><meta property="og:description" content="本記事では，主に用語の確認をする．
なお，強化学習において頻繁に登場する記号たちをExtraMaterialにまとめている．
エージェントと環境 エージェントは，強化学習における主人公的なポジションであり， 行動(Action) をする．エージェントの行動によって，変化するものが 環境(Environment) である．エージェントは，環境の 状態(State) を得ることによって，次の行動を決定する．行動によって評価されるものが 報酬(Reward) である．
まとめると，時刻$t$のとき次の手順を踏む．
エージェントは環境から状態$S_t$を得る． エージェントは状態$S_t$を用いて行動$A_t$を決定する． エージェントは行動$A_t$を実行する． エージェントは環境から報酬$R_t$を得る． 時刻$t+1$に移る． 決定的，確率的挙動 ロボット(エージェント)が迷路の中で，右に移動するという行動を取った．しかし，たまたま床がツルツルで滑ってしまうことが原因で，$0.1$の確立で左に移動してしまうとする．あるいは，エージェント以外が状態を変化させるような問題も考えられる．このような，行動に対して状態が確率によって変化することを 確率的(stochastic) な挙動 と呼ぶ．逆に，行動が確率によって変化しないことを 決定的(deterministic) な挙動 と呼ぶ．
決定的な挙動は式($1$)で表される． $$ \begin{align} s^\prime = f(s,a) \end{align} $$ $f$は状態$s$と行動$a$を受け取り，次の状態$s^\prime$を返す関数である．
また，状態$s$と行動$a$を取って状態$s^\prime$が得られる確率を式($2$)の$p$で表記する． $$ \begin{align} p(s^\prime|s,a) \end{align} $$ $p(s^\prime|s,a)$のパイプラインのような記号$|$を用いた表記は，$p(s^\prime)$の$s^\prime$に条件$s,a$を付け足すものである． 例えば，迷路の例で，
$s$=スタート地点 $a$=上に進む $s^\prime$=座標1に移動する の場合$p(s^\prime|s,a)$は「スタート地点にいて上に進んだときに，座標1に移動する確率」を表している．
なお，決定的な挙動の場合，$p(s^\prime|s,a)=1$であると考えれば確率的な挙動の特殊ケースと捉えられる．
マルコフ性 $p(s^\prime|s,a)$という式は，次の状態$s^\prime$が現在の状態$s$と行動$a$のみから決まることを意味している．このような性質を マルコフ性(Markov property) と呼ぶ． 別の言い方をすると，過去の行動や状態は考えなくて良いということである．このマルコフ性を仮定することにより，過去の行動を考えた膨大な行動パターンを考える必要がなくなり，問題が解きやすくなる．
方策 マルコフ性を仮定した場合，現在の状態$s$のみから次の行動$a$を決定する．行動の方針のことを 方策(policy) と呼ぶ． 状態$s$であるときに，行動$a$を取る確率を$\pi(a|s)$で表現する． 決定論的な場合は$a=\mu(s)$と表すこともある．
エピソードタスクと連続タスク エピソードタスクはオセロやチェスのような終わりのある問題のことである．オセロであれば，最終的には勝ち負け引き分けのいずれかになる．対して，終わりが考えにくいような問題を連続タスクと呼ぶ．注文を確認して在庫を管理するような問題は，これといった終わりを考えないエンドレスなものだと認識すると連続タスクだと考えられる．
収益 エージェントは時刻$t$から報酬$R_t,R_{t+1},R_{t+2},\cdots$を得ていく．これらの報酬をあわせたものを収益と呼ぶ． 時刻$t$の収益$G_t$は具体的に式($3$)の計算をする．ただし，$\gamma$は割引率と呼び，$0\leq \gamma \leq 1$である．
$$ \begin{align} G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \cdots \end{align} $$"><meta property="og:url" content="https://shibaken28.github.io/my-blog-4/contents/rl-1/"><meta property="og:site_name" content="shibak3n's blog"><meta property="og:type" content="article"><meta property="article:section" content="Post"><meta property="article:published_time" content="2023-01-06T21:44:36+09:00"><meta property="article:modified_time" content="2023-01-06T21:44:36+09:00"><meta name=twitter:title content="強化学習勉強メモ #1 用語確認"><meta name=twitter:description content="本記事では，主に用語の確認をする．
なお，強化学習において頻繁に登場する記号たちをExtraMaterialにまとめている．
エージェントと環境 エージェントは，強化学習における主人公的なポジションであり， 行動(Action) をする．エージェントの行動によって，変化するものが 環境(Environment) である．エージェントは，環境の 状態(State) を得ることによって，次の行動を決定する．行動によって評価されるものが 報酬(Reward) である．
まとめると，時刻$t$のとき次の手順を踏む．
エージェントは環境から状態$S_t$を得る． エージェントは状態$S_t$を用いて行動$A_t$を決定する． エージェントは行動$A_t$を実行する． エージェントは環境から報酬$R_t$を得る． 時刻$t+1$に移る． 決定的，確率的挙動 ロボット(エージェント)が迷路の中で，右に移動するという行動を取った．しかし，たまたま床がツルツルで滑ってしまうことが原因で，$0.1$の確立で左に移動してしまうとする．あるいは，エージェント以外が状態を変化させるような問題も考えられる．このような，行動に対して状態が確率によって変化することを 確率的(stochastic) な挙動 と呼ぶ．逆に，行動が確率によって変化しないことを 決定的(deterministic) な挙動 と呼ぶ．
決定的な挙動は式($1$)で表される． $$ \begin{align} s^\prime = f(s,a) \end{align} $$ $f$は状態$s$と行動$a$を受け取り，次の状態$s^\prime$を返す関数である．
また，状態$s$と行動$a$を取って状態$s^\prime$が得られる確率を式($2$)の$p$で表記する． $$ \begin{align} p(s^\prime|s,a) \end{align} $$ $p(s^\prime|s,a)$のパイプラインのような記号$|$を用いた表記は，$p(s^\prime)$の$s^\prime$に条件$s,a$を付け足すものである． 例えば，迷路の例で，
$s$=スタート地点 $a$=上に進む $s^\prime$=座標1に移動する の場合$p(s^\prime|s,a)$は「スタート地点にいて上に進んだときに，座標1に移動する確率」を表している．
なお，決定的な挙動の場合，$p(s^\prime|s,a)=1$であると考えれば確率的な挙動の特殊ケースと捉えられる．
マルコフ性 $p(s^\prime|s,a)$という式は，次の状態$s^\prime$が現在の状態$s$と行動$a$のみから決まることを意味している．このような性質を マルコフ性(Markov property) と呼ぶ． 別の言い方をすると，過去の行動や状態は考えなくて良いということである．このマルコフ性を仮定することにより，過去の行動を考えた膨大な行動パターンを考える必要がなくなり，問題が解きやすくなる．
方策 マルコフ性を仮定した場合，現在の状態$s$のみから次の行動$a$を決定する．行動の方針のことを 方策(policy) と呼ぶ． 状態$s$であるときに，行動$a$を取る確率を$\pi(a|s)$で表現する． 決定論的な場合は$a=\mu(s)$と表すこともある．
エピソードタスクと連続タスク エピソードタスクはオセロやチェスのような終わりのある問題のことである．オセロであれば，最終的には勝ち負け引き分けのいずれかになる．対して，終わりが考えにくいような問題を連続タスクと呼ぶ．注文を確認して在庫を管理するような問題は，これといった終わりを考えないエンドレスなものだと認識すると連続タスクだと考えられる．
収益 エージェントは時刻$t$から報酬$R_t,R_{t+1},R_{t+2},\cdots$を得ていく．これらの報酬をあわせたものを収益と呼ぶ． 時刻$t$の収益$G_t$は具体的に式($3$)の計算をする．ただし，$\gamma$は割引率と呼び，$0\leq \gamma \leq 1$である．
$$ \begin{align} G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \cdots \end{align} $$"><script async src="https://www.googletagmanager.com/gtag/js?id=G-KXVWMJ76L6"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KXVWMJ76L6")</script><link href="https://fonts.googleapis.com/css?family=Noto+Sans+JP" rel=stylesheet><style>:root{--ja-font-family:"Noto Sans JP",'ヒラギノ角ゴ Pro W3', 'Hiragino Kaku Gothic Pro', '游ゴシック', 'Yu Gothic', 'ＭＳ Ｐゴシック', 'MS PGothic', sans-serif;--base-font-family:"Lato", var(--sys-font-family), var(--ja-font-family), sans-serif;--article-font-size:12pt}.article-content .katex-display>.katex{overflow-x:hidden;overflow-y:hidden}</style></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label=メニューを開く・閉じる>
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><div class=site-meta><h1 class=site-name><a href=/my-blog-4>shibak3n's blog</a></h1><h2 class=site-description>趣味全般</h2></div></header><ol class=social-menu><li><a href=https://github.com/Shibaken28 target=_blank title=GitHub rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=https://steamcommunity.com/profiles/76561199155194438/ target=_blank title=steam rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg></a></li><li><a href=https://twitter.com/Shibak33333333n target=_blank title=Twitter rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-twitter" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M22 4.01c-1 .49-1.98.689-3 .99-1.121-1.265-2.783-1.335-4.38-.737S11.977 6.323 12 8v1c-3.245.083-6.135-1.395-8-4 0 0-4.182 7.433 4 11-1.872 1.247-3.739 2.088-6 2 3.308 1.803 6.913 2.423 10.034 1.517 3.58-1.04 6.522-3.723 7.651-7.742a13.84 13.84.0 00.497-3.753C20.18 7.773 21.692 5.25 22 4.009z"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/my-blog-4/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg><span>Home</span></a></li><li><a href=/my-blog-4/archives/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg><span>Archives</span></a></li><li><a href=/my-blog-4/search/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg><span>Search</span></a></li><li><a href=/my-blog-4/links/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg><span>Links</span></a></li><div class=menu-bottom-section><li id=dark-mode-toggle><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><span>ダークモード</span></li></div></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">目次</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#エージェントと環境>エージェントと環境</a></li><li><a href=#決定的確率的挙動>決定的，確率的挙動</a></li><li><a href=#マルコフ性>マルコフ性</a></li><li><a href=#方策>方策</a></li><li><a href=#エピソードタスクと連続タスク>エピソードタスクと連続タスク</a></li><li><a href=#収益>収益</a></li><li><a href=#状態価値関数>状態価値関数</a></li><li><a href=#行動価値関数>行動価値関数</a></li></ol></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/my-blog-4/categories/%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92/>強化学習</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/my-blog-4/contents/rl-1/>強化学習勉強メモ #1 用語確認</a></h2></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg><time class=article-time--published>Jan 06, 2023</time></div></footer></div></header><section class=article-content><p>本記事では，主に用語の確認をする．</p><p>なお，強化学習において頻繁に登場する記号たちを<a class=link href=/my-blog-4/contents/rl-ex/>ExtraMaterial</a>にまとめている．</p><h2 id=エージェントと環境>エージェントと環境</h2><p>　<strong>エージェント</strong>は，強化学習における主人公的なポジションであり， <strong>行動(Action)</strong> をする．エージェントの行動によって，変化するものが <strong>環境(Environment)</strong> である．エージェントは，環境の <strong>状態(State)</strong> を得ることによって，次の行動を決定する．行動によって評価されるものが <strong>報酬(Reward)</strong> である．</p><p>　まとめると，時刻$t$のとき次の手順を踏む．</p><ol><li>エージェントは環境から状態$S_t$を得る．</li><li>エージェントは状態$S_t$を用いて行動$A_t$を決定する．</li><li>エージェントは行動$A_t$を実行する．</li><li>エージェントは環境から報酬$R_t$を得る．</li><li>時刻$t+1$に移る．</li></ol><h2 id=決定的確率的挙動>決定的，確率的挙動</h2><p>　ロボット(エージェント)が迷路の中で，右に移動するという行動を取った．しかし，たまたま床がツルツルで滑ってしまうことが原因で，$0.1$の確立で左に移動してしまうとする．あるいは，エージェント以外が状態を変化させるような問題も考えられる．このような，行動に対して状態が確率によって変化することを <strong>確率的(stochastic)</strong> な挙動 と呼ぶ．逆に，行動が確率によって変化しないことを <strong>決定的(deterministic)</strong> な挙動 と呼ぶ．</p><p>決定的な挙動は式($1$)で表される．
$$
\begin{align}
s^\prime = f(s,a)
\end{align}
$$
$f$は状態$s$と行動$a$を受け取り，次の状態$s^\prime$を返す関数である．</p><p>また，状態$s$と行動$a$を取って状態$s^\prime$が得られる確率を式($2$)の$p$で表記する．
$$
\begin{align}
p(s^\prime|s,a)
\end{align}
$$
$p(s^\prime|s,a)$のパイプラインのような記号$|$を用いた表記は，$p(s^\prime)$の$s^\prime$に条件$s,a$を付け足すものである．
例えば，迷路の例で，</p><ul><li>$s$=スタート地点</li><li>$a$=上に進む</li><li>$s^\prime$=座標1に移動する</li></ul><p>の場合$p(s^\prime|s,a)$は「スタート地点にいて上に進んだときに，座標1に移動する確率」を表している．</p><p>なお，決定的な挙動の場合，$p(s^\prime|s,a)=1$であると考えれば確率的な挙動の特殊ケースと捉えられる．</p><h2 id=マルコフ性>マルコフ性</h2><p>　$p(s^\prime|s,a)$という式は，次の状態$s^\prime$が現在の状態$s$と行動$a$のみから決まることを意味している．このような性質を <strong>マルコフ性(Markov property)</strong> と呼ぶ．
別の言い方をすると，<strong>過去の行動や状態は考えなくて良い</strong>ということである．このマルコフ性を仮定することにより，過去の行動を考えた膨大な行動パターンを考える必要がなくなり，問題が解きやすくなる．</p><h2 id=方策>方策</h2><p>　マルコフ性を仮定した場合，現在の状態$s$のみから次の行動$a$を決定する．行動の方針のことを <strong>方策(policy)</strong> と呼ぶ．
状態$s$であるときに，行動$a$を取る確率を$\pi(a|s)$で表現する．
決定論的な場合は$a=\mu(s)$と表すこともある．</p><h2 id=エピソードタスクと連続タスク>エピソードタスクと連続タスク</h2><p>　エピソードタスクはオセロやチェスのような終わりのある問題のことである．オセロであれば，最終的には勝ち負け引き分けのいずれかになる．対して，終わりが考えにくいような問題を連続タスクと呼ぶ．注文を確認して在庫を管理するような問題は，これといった終わりを考えないエンドレスなものだと認識すると連続タスクだと考えられる．</p><h2 id=収益>収益</h2><p>　エージェントは時刻$t$から報酬$R_t,R_{t+1},R_{t+2},\cdots$を得ていく．これらの報酬をあわせたものを収益と呼ぶ．
時刻$t$の収益$G_t$は具体的に式($3$)の計算をする．ただし，$\gamma$は割引率と呼び，$0\leq \gamma \leq 1$である．</p><p>$$
\begin{align}
G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \cdots
\end{align}
$$</p><p>割引率を導入することで，未来の報酬を現在の報酬に対してどれだけ重視するかを決めることができる．また，連続タスクでは，$\gamma$を導入しないと$G_t$は無限大に発散するので，$G_t$を有限値に収束させるためにも必要である．</p><h2 id=状態価値関数>状態価値関数</h2><p>　収益$G_t$を導入したが，$G_t$は未来の報酬について値を計算するため，報酬$R$の値が不明である．未来では確率的な行動をすることもあり，どの値を報酬として用いるかがわからない．ここで，式($4$)で表される<strong>状態価値関数</strong>を導入する．</p><p>$$
\begin{align}
v_\pi(s) = \mathbb{E}_\pi[G_t|S_t=s]
\end{align}
$$</p><p>さて，多くの変数が出てきてのでこれらの意味を整理する．</p><ul><li>$v_\pi(s)$は状態$s$の価値を表している．</li><li>$v$や$\mathbb{E}$の右下についている$\pi$はこの方策ですよというサインみたいなもの．<ul><li>条件として$G_t|\pi$という表記をする場合もある．</li></ul></li><li>$\mathbb{E} [ X ]$は$X$の期待値．<ul><li>$\mathbb{E}_\pi[G_t|S_t=s]$は，状態$s$にいて，方策$\pi$を用いたときの収益$G_t$の期待値を表している． なぜ期待値であるのかというと，方策$\pi$は確率で様々な行動$a$を取るため，それらの行動を平均化(期待値)したものを収益として用いるためである．</li></ul></li></ul><p>方策$\pi$をうまく選ぶことで，他のどの方策よりも状態$s$の価値が高くなるようにすることができる<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>．そのような方策を <strong>最適方策</strong> と呼び，$\pi^*$と表す．また，$\pi^*$についての状態価値観数$v_{\pi^*}$($v_*$と表すこともある)を <strong>最適状態価値関数</strong> と呼ぶ．</p><h2 id=行動価値関数>行動価値関数</h2><p>　状態価値関数$v$は，ある状態$s$の価値を表していた．行動価値関数$q$は，ある状態$s$で，行動$s$をしたときの価値を表す．定義式は，状態価値関数に行動の要素を追加させた式$(5)$で表される．</p><p>$$
\begin{align}
q_\pi(s,a) = \mathbb{E}_\pi[G_t|S_t=s,A_t=a]
\end{align}
$$</p><p>単に$Q$関数と呼ぶこともある．</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>証明が可能だそう．&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></section><footer class=article-footer><section class=article-copyright><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg><span>Licensed under CC BY-NC-SA 4.0</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/katex.min.css integrity="sha256-J+iAE0sgH8QSz9hpcDxXIftnj65JEZgNhGcgReTTK9s=" crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/katex.min.js integrity="sha256-InsNdER1b2xUewP+pKCUJpkhiqwHgqiPXDlIk7GzBu4=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/contrib/auto-render.min.js integrity="sha256-y39Mpg7V3D4lhBX4x6O0bUqTV4pSrfgwEfGKfxkOdgI=" crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{renderMathInElement(document.querySelector(`.article-content`),{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><aside class=related-content--wrapper><h2 class=section-title>関連するコンテンツ</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/my-blog-4/contents/puyo-dqn/><div class=article-details><h2 class=article-title>強化学習でぷよぷよを学習させたかった(願望)</h2></div></a></article><article><a href=/my-blog-4/contents/rl-0/><div class=article-details><h2 class=article-title>強化学習勉強メモ (目次)</h2></div></a></article></div></div></aside><footer class=site-footer><section class=copyright>&copy;
2022 -
2024 shibak3n's blog</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>テーマ <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.21.0>Stack</a></b> は <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a> によって設計されています。</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/my-blog-4/ts/main.js defer></script>
<script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>
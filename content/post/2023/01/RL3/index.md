---
title: "三目並べを様々な強化学習で実装 #3 動的計画法"
description: 
slug: "RL-3"
date: 2023-01-06T22:01:36+09:00
categories:
    - 強化学習
tags:
image: 
---

## 反復方策評価
ベルマン方程式を使って実際に各$s$における状態価値関数$v(s)$の値を求めることは，状態数が増えていくにつれ，難しくなる．そのため，$v(s)$の値をより正確な値に近づけていくような，値を**更新**させていくよな，手法を考える．

もう一度，行動価値関数のベルマン方程式を振り返る．
$$
\begin{align}
v\_\pi(s)
&= \sum \_{a}  \sum \_{s'}\pi(a|s) p(s'|s,a) \left\\{ r(s,a,s') + \gamma v\_\pi(s') \right\\}
\end{align}
$$

このベルマン方程式を，更新の関数に置き換える．
$k$回目の更新を行った状態価値関数$V_{k}$から，$k+1$回目の更新によって得る状態価値関数を$V_{k+1}$とすると，

$$
\begin{align}
V\_{k+1}(s)
&= \sum \_{a}  \sum \_{s'}\pi(a|s) p(s'|s,a) \left\\{ r(s,a,s') + \gamma V\_{k}(s') \right\\}
\end{align}
$$

となる．ただし，漸化式であるため，適当な初期の関数$V_{0}$を与える必要がある．
方策$\pi$を使い，$V_{0}$をもとにして，$V_{1}$を求め，$V_{1}$をもとにして$V_{2}$を求め，というように，$V_{k}$を求めていく．そして，収束したときにそれが状態価値関数$v_\pi (s)$になる[^1]．

この手法を**反復方策評価**と呼ぶ

[^1]:証明は省略する．


## 方策反復法
反復方策評価を使うと，方策$\pi$に対する状態価値関数$v_\pi (s)$を求めることができるが，本当に欲しいものは**最適な**方策$\pi_\*$と，その方策に対する状態価値関数$v\_\*(s)$である．

確率的方策$\pi$から，最適な方策$\pi_\*$へ近づける鍵は，決定論的方策に変換することである．
具体的には，$v_\pi (s)$をもとに，$s$における最適な行動$a_\*$を選択する．
そして，その行動をとる確率を1に，それ以外の行動をとる確率を0にする．これにより，決定論的方策$\mu(s)$に変換される．数式で表すと，次のようになる．
$$
\begin{align}
\mu(s) &= \text{argmax} \_{a} q (s,a) \\\\
&= \text{argmax} \_{a} \sum \_{s'} p(s'|s,a) \left\\{ r(s,a,s') + \gamma v (s') \right\\}
\end{align}
$$
$\mu(s)$は，$v(s)$から得られた，貪欲的な方策，**greedy** policyと呼ばれる．**greedy 化**と呼ばれることもある．
先程説明した反復方策評価とgreedy化を交互に繰り返すと，greedy化しても方策が変わらなくなる地点に到達する．それが，最適方策となる．このような手法を**方策反復法**と呼ぶ．

## 価値反復法
**価値反復法**は，方策反復法で別々に行っていた反復方策評価とgreedy化を同時に行う手法である．式で表すと，次のようになる．
$$
\begin{align}
V\_{k+1}(s) = \max_{a} \sum \_{s'} p(s'|s,a) \left\\{ r(s,a,s') + \gamma V\_{k} (s') \right\\}
\end{align}
$$
これによって更新がストップした時点で，greedy化を行えば最適方策$\mu\_*(s)$が得られる．

